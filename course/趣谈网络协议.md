## [趣谈网络协议](https://time.geekbang.org/column/intro/85) 刘超
#course #network 

- 内容以下主要架构，未涉及部分在本页面
  - [network](../network/network) 
  - [TCP/IP](../network/tcp_ip)
- 协议三要素
  - 语法 符合一定规则和格式
  - 语义 代表某种意义
  - 顺序
- 分层
  - **只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层**
  - 下层是上层实现的基础，下层要包含上层的语义
  - 完整数据包要经过 MAC 传输
- IP vs MAC 二层会依赖四层，四层也会依赖二层

## DC

- 每一个连接都要考虑高可用
- 服务器
  - 服务器被放在一个个机架 Rack
  - 至少两个网卡、两个网线插到 TOR 交换机上，
  - 网卡绑定 bond 两个网卡要工作得像一张网卡一样
    - 需要服务器和交换机都支持 LACP（Link Aggregation Control Protocol）
      - 互相通信，将多个网卡聚合称为一个网卡，多个网线聚合成一个网线，在网线之间可以进行负载均衡，也可以为了高可用作准备。
- 接入层 Access Layer 一层 TOR 交换机
  - TOR（Top Of Rack）交换机  放在机架顶端的交换机将服务器连接起来，可以互相通信
- 汇聚层交换机 Aggregation Layer 多个机架连接在一起，对性能的要求更高，带宽也更大
- 交换机高可用
  - 部署两个接入交换机、两个汇聚交换机
    - 服务器和两个接入交换机都连接，接入交换机和两个汇聚都连接，当然这样会形成环，所以需要启用 STP 协议，去除环，但是这样两个汇聚就只能一主一备
  - 堆叠 多个交换机形成一个逻辑的交换机
    - 服务器通过多根线分配连到多个接入层交换机上，接入层交换机多根线分别连接到多个交换机上，并且通过堆叠的私有协议，形成双活的连接方式
    - 由于对带宽要求更大，而且挂了影响也更大，所以两个堆叠可能就不够了，可以就会有更多的，比如四个堆叠为一个逻辑的交换机。
- POD（Point Of Delivery）|可用区（Available Zone）
  - 汇聚层将大量计算节点相互连接在一起，形成一个集群。集群里面，服务器之间通过二层互通
 
- 节点数目再多的时候，一个可用区放不下，用**核心交换机**将多个可用区连在一起
  - 核心交换机吞吐量更大，高可用要求更高，肯定需要堆叠
  - 往往仅仅堆叠，不足以满足吞吐量，因而还是需要部署多组核心交换机。
  - 核心和汇聚交换机之间为了高可用，也是全互连模式的
- 核心和汇聚交换机环路问题
  - 不同可用区在不同二层网络，分配不同网段。汇聚和核心之间通过三层网络互通，二层都不在一个广播域里面，不会存在二层环路的问题。三层有环是没有问题的，只要通过路由协议选择最佳的路径就可以了
    - 二层是广播域，会存在“循环广播风暴”，所以二层不能有环路
    - 核心层和汇聚层之间通过内部的路由协议 OSPF，找到最佳的路径进行访问，而且还可以通过 ECMP 等价路由，在多个路径之间进行负载均衡和高可用
    
- 云计算、大数据，集群规模非常大，而且都要求在一个二层网络里面。
  - 二层互连从汇聚层上升为核心层，也即在核心以下，全部是二层互连，全部在一个广播域里面，这就是常说的大二层（局域网组网）
  - 如果大二层横向流量不大，核心交换机数目不多，可以做堆叠，但是如果横向流量很大，仅仅堆叠满足不了，就需要部署多组核心交换机，而且要和汇聚层进行全互连。
  - 由于堆叠只解决一个核心交换机组内的无环问题，而组之间全互连，还需要其他机制进行解决
  - 如果是 STP，那部署多组核心无法扩大横向流量的能力，因为还是只有一组起作用。
  - 大二层就引入 TRILL（Transparent Interconnection of Lots of Link）多链接透明互联协议
    - 基本思想 二层环有问题，三层环没有问题，那就把三层的路由能力模拟在二层实现。
    - 运行 TRILL 协议的交换机称为 RBridge，具有路由转发特性的网桥设备，只不过这个路由是根据 MAC 地址来的，不是根据 IP 来的。
    - Rbridage 之间通过链路状态协议运作。通过它可以学习整个大二层的拓扑，知道访问哪个 MAC 应该从哪个网桥走；还可以计算最短的路径，也可以通过等价的路由进行负载均衡和高可用性。
    - TRILL 协议在原来 MAC 头外面加上自己的头，以及外层的 MAC 头。TRILL 头里面的 Ingress RBridge，有点像 IP 头里面的源 IP 地址，Egress RBridge 是目标 IP 地址，这两个地址是端到端的，在中间路由的时候，不会发生改变。而外层的 MAC，可以有下一跳的 Bridge，就像路由的下一跳，也是通过 MAC 地址来呈现的一样。
    - 对于大二层的广播包，也需要通过分发树的技术来实现。STP 是将一个有环的图，通过去掉边形成一棵树，而分发树是一个有环的图形成多棵树，不同的树有不同的 VLAN，有的广播包从 VLAN A 广播，有的从 VLAN B 广播，实现负载均衡和高可用。
- 在核心交换上面，往往会挂一些安全设备，例如入侵检测、DDoS 防护等等。这是整个数据中心的屏障，防止来自外来的攻击。核心交换机上往往还有负载均衡器
- 边界路由器 Border Router 在数据中心的边界,数据中心的入口和出口也是路由器
  - 为了高可用,会连接多个运营商网络
  - 多线 BGP
    - 数据中心往往就是路由协议中的自治区域（AS）
    - 数据中心里面机器要想访问外面网站，数据中心里面也是有对外提供服务的机器，都可以通过 BGP 协议，获取内外互通的路由信息
- 整个数据中心的网络结构
	- 三层网络结构 接入层、汇聚层、核心层
	- 南北流量 从外到内或者从内到外，对应到图里，就是从上到下，从下到上，上北下南
	- 东西流量 但是随着云计算和大数据的发展，节点之间的交互越来越多，例如大数据计算经常要在不同的节点将数据拷贝来拷贝去，这样需要经过交换机，使得数据从左到右，从右到左，左西右东
	- 叶脊网络（Spine/Leaf）解决东西流量的问题
		- 叶子交换机（leaf），直接连接物理服务器。L2/L3 网络的分界点在叶子交换机上，叶子交换机之上是三层网络。
		- 脊交换机（spine switch），相当于核心交换机。叶脊之间通过 ECMP 动态选择多条路径。脊交换机现在只是为叶子交换机提供一个弹性的 L3 路由网络。南北流量可以不用直接从脊交换机发出，而是通过与 leaf 交换机并行的交换机，再接到边界路由器出去。
- 运维人员通过 vpn 连入机房网络，再通过堡垒机访问服务器或网络设备

![数据中心架构](../_static/dc_arch.png)
![数据中心大集群架构](../_static/dc_large.png)
![TRILL 帧结构](../_static/trill_packet.png)
![整个数据中心的网络](../_static/dc_arch_final.png)
![叶脊网络](../_static/spine.png)

## vpn Virtual Private Network 虚拟专用网

- 通过隧道技术在公众网络上仿真一条点到点的专线
- 将一个机构的多个数据中心通过隧道的方式连接起来，让机构感觉在一个数据中心里面
- 通过利用一种协议来传输另外一种协议的技术,涉及三种协议：乘客协议(本地化)=>隧道协议(路径)=>承载协议(真实数据)

### IPsec VPN 基于 IP 协议的安全隧道协议

- 根据封装网络包的格式分两种协议
	- AH（Authentication Header）只能进行数据摘要 ，不能实现数据加密
	- ESP（Encapsulating Security Payload）能够进行数据加密和数据摘要
- 加密算法
- 摘要算法
- DH 算法
	- 一个比较巧妙的算法
	- 客户端和服务端约定两个公开的质数 p 和 q，然后客户端随机产生一个数 a 作为自己的私钥，服务端随机产生一个 b 作为自己的私钥，
	- 客户端可以根据 p、q 和 a 计算出公钥 A，服务端根据 p、q 和 b 计算出公钥 B，然后双方交换公钥 A 和 B。
	- 客户端和服务端可以根据已有的信息，各自独立算出相同的结果 K，就是对称密钥
-  IKE 组件  用于 VPN 双方进行对称密钥的交换
-  SA（Security Association）组件 VPN 双方连接进行维护
- 过程
	- 建立 IKE 自己的 SA
		- 这个 SA 用来维护一个通过身份认证和安全保护的通道，为第二个阶段提供服务。
		- 通过 DH（Diffie-Hellman）算法计算出一个对称密钥 K。这个过程，对称密钥从来没有在通道上传输过，只传输了生成密钥的材料，通过这些材料，截获的人是无法算出的。
	- 建立 IPsec SA
		- 在这个 SA 里面，双方会生成一个随机对称密钥 M，由 K 加密传给对方，使用 M 进行双方接下来通信的数据
		- 对称密钥 M 是有过期时间的，过一段时间，重新生成一次，防止被破解。
		- 内容
			- SPI（Security Parameter Index），用于标识不同的连接
			- 双方商量好的加密算法、哈希算法和封装模式
			- 生存周期，超过这个周期，就需要重新生成一个 IPsec SA，重新生成对称密钥。
	- 可以打包封装传输
		- 左面 原始的 IP 包，在 IP 头里面，会指定上一层的协议为 TCP。
		- ESP 要对 IP 包进行封装，因而 IP 头里面的上一层协议为 ESP。
		- ESP 正文里面，ESP 的头部有双方商讨好的 SPI，以及这次传输的序列号
		- 接下来全部是加密的内容。
	- 隧道出来后解封装
		- 通过对称密钥进行解密，解密后在正文最后，指明了里面的协议是什么
		- 如果是 IP，则需要先解析 IP 头，然后解析 TCP 头
- 有了 IPsec VPN 之后，客户端发送的明文 IP 包，都会被加上 ESP 头和 IP 头，在公网上传输，由于加密，可以保证不被窃取，到对端后，去掉 ESP 的头，进行解密。
- 这种点对点的基于 IP 的 VPN，能满足互通的要求，但是速度往往比较慢，这是由底层 IP 协议的特性决定的。
	- IP 不是面向连接的，是尽力而为的协议，每个 IP 包自由选择路径，到每一个路由器，都自己去找下一跳，丢了就丢了，靠上一层 TCP 的重发来保证可靠性。
	- 因为 IP 网络从设计的时候，就认为是不可靠的，所以即使同一个连接，也可能选择不同的道路，这样的好处是，一条道路崩溃的时候，总有其他的路可以走。当然，带来的代价就是，不断的路由查找，效率比较差。
- 另一种技术协议 ATM
	- 和 IP 协议的不同在于，是面向连接的
	- 传输之前先建立一个连接，形成一个虚拟的通路，一旦连接建立，所有的包都按照相同的路径走，不会分头行事
	- 好处 不需要每次都查路由表的，虚拟路径已经建立，打上标签，后续包跟着走就是了，不用像 IP 包一样，每个包都思考下一步怎么走，都按相同的路径走，这样效率会高很多。
	- 一旦虚拟路径上某个路由器坏了，则这个连接就断了，什么也发不过去了，因为其他的包还会按照原来的路径走，都掉坑里了，不会选择其他的路径走
	- 虽然没有成功，但其屏弃繁琐的路由查找，改为简单快速的标签交换，将具有全局意义的路由表改为只有本地意义的标签表，这些都可以大大提高一台路由器的转发功力。
- 多协议标签交换 MPLS，Multi-Protocol Label Switching
	- 在原始 IP 头之外，多了 MPLS 头，里面可以打标签
	-  二层头里面，类型字段 0x0800 表示 IP，0x8847 表示 MPLS Label
	-  MPLS 头 结构
		- 标签值占 20 位
		- 3 位实验位
		- 1 位栈底标志位，表示当前标签是否位于栈底了。这样就允许多个标签被编码到同一个数据包中，形成标签栈。
		- 8 位 TTL 存活时间字段，如果标签数据包的出发 TTL 值为 0，那么该数据包在网络中的生命期被认为已经过期了。
	- 标签交换路由器（LSR，Label Switching Router）
		- 转发标签的路由设备，认这个标签，并且能够根据这个标签转发，有两个表格
			- 传统的 FIB，也即路由表
			- LFIB，标签转发表
		- 在 MPLS 区域中间，使用标签进行转发
		- 非 MPLS 区域，使用普通路由转发
		- 在边缘节点上，需要有能力将对于普通路由转发变成对于标签的转发。
		- LSP 通过标签转换而建立的路径 
			- 要访问 114.1.1.1，在边界上查找普通路由，发现马上要进入 MPLS 区域了，进去了对应标签 1，于是在 IP 头外面加一个标签 1
			- 在区域里面，标签 1 要变成标签 3
			- 标签 3 到达出口边缘，将标签去掉，按照路由发出
			- 在一条 LSP 上，沿数据包传送方向，相邻 LSR 分别叫上游 LSR（upstream LSR）和下游 LSR（downstream LSR）。
		- LDP（Label Distribution Protocol） 动态的生成标签的协议
			- 标签分发 通过 LSR 的交互，互相告知去哪里应该打哪个标签
			- 往往是从下游开始的
			- 如果有一个边缘节点发现自己的路由表中出现了新的目的地址，它就要给别人说，能到达一条新的路径了。
			- 如果此边缘节点存在上游 LSR，并且尚有可供分配的标签，则该节点为新的路径分配标签，并向上游发出标签映射消息，其中包含分配的标签等信息。
			- 收到标签映射消息的 LSR 记录相应的标签映射信息，在其标签转发表中增加相应的条目。此 LSR 为它的上游 LSR 分配标签，并继续向上游 LSR 发送标签映射消息。
			- 当入口 LSR 收到标签映射消息时，在标签转发表中增加相应的条目。这时，就完成了 LSP 的建立。
	- MPLS VPN 中，网络中的路由器分类
		- CE（Customer Edge）：客户网络与 PE 相连接的边缘设备
		- PE（Provider Edge）：运营商网络与客户网络相连的边缘网络设备
		- P（Provider）：特指运营商网络中除 PE 之外的其他运营商网络设备
		- P Router 之间，使用标签是没有问题的，因为都在运营商的管控之下，对于网段，路由都可以自己控制
		- 客户接入网络，客户地址重复
			- BGP 协议处理 PE 路由器之间使用特殊的 MP-BGP 来发布 VPN 路由，在相互沟通的消息中，在一般 32 位 IPv4 的地址之前加上一个客户标示的区分符用于客户地址的区分，这种称为 VPN-IPv4 地址族，这样 PE 路由器会收到如下的消息，机构 A 的 192.168.101.0/24 应该往这面走，机构 B 的 192.168.101.0/24 则应该去另外一个方向。
			- 路由表处理 在 PE 上，可以通过 VRF（VPN Routing&Forwarding Instance）建立每个客户一个路由表，与其它 VPN 客户路由和普通路由相互区分。可以理解为专属于客户的小路由器。远端 PE 通过 MP-BGP 协议把业务路由放到近端 PE，近端 PE 根据不同的客户选择出相关客户的业务路由放到相应的 VRF 路由表中。
		- VPN 报文转发采用两层标签方式
			- 第一层（外层）标签在骨干网内部进行交换，指示从 PE 到对端 PE 的一条 LSP。VPN 报文利用这层标签，可以沿 LSP 到达对端 PE
			- 第二层（内层）标签在从对端 PE 到达 CE 时使用，在 PE 上，通过查找 VRF 表项，指示报文应被送到哪个 VPN 用户，或者更具体一些，到达哪一个 CE。这样，对端 PE 根据内层标签可以找到转发报文的接口。

![IPsec 包结构](../_static/ipsec_frame_struct.jpg)
![MPLS VPN 网络](../_static/mpls_net.png)

## 云计算

- 灵活：虚拟机和物理不同，会经常创建、删除，从一个机器漂移到另一台机器，有的互通、有的不通等等，灵活性比物理网络要好得多，需要能够灵活配置。

### 共享与互通

- 共享：每个虚拟机都会有一个或者多个虚拟网卡，物理机上可能只有有限网卡。多虚拟网卡如何共享同一个出口？
- 互通
	- 同一台机器上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？
	- 不同物理机上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？

##### 同物理机互通

- 在物理机上，应该有一个虚拟交换机，在 Linux 上有一个命令叫作 brctl，可以创建虚拟网桥
	- 电脑上发现多了几个网卡,是虚拟交换机
- 将两个虚拟机虚拟网卡，都连接到虚拟网桥上
- 将两个虚拟机配置相同子网网段

#### 桥接 

- 物理网卡连接到虚拟交换机上
- 登录虚拟机里看 IP 地址会发现，虚拟机地址和物理机，以及同一网段局域网主机
- 将物理机和虚拟机放在同一个网桥上，相当于网桥上机器在一个网段，全部打平
- 在数据中心里面，采取也是类似的技术，所有 br0 都通过物理网卡出来连接到物理交换机上
- 物理交换机连接各个物理机上虚拟网桥，所有设备都在二层网络。解决跨物理机的互通问题。
	- 广播问题严重，需要通过 VLAN 进行划分
	- 桌面虚拟机或者数据中心规模非常小，才可以使用这种相对简单的方式

```sh
brctl addbr br0
brctl addif br0 tap0
```

#### NAT 模式

- 登录虚拟机查看 IP 地址，会发现虚拟机的网络是虚拟机的，物理机的网络是物理机的，两个不相同。
- 虚拟机要想访问物理机的时候，需要将地址 NAT 成为物理机的地址。
- 物理机内置一个 DHCP 服务器，为物理机上虚拟机动态分配 IP 地址
- 手动配置
	- 将虚拟机所在网络网关地址直接配置到 br0 上，不用 DHCP Server，手动配置每台虚拟机的 IP 地址，通过命令 iptables -t nat -A POSTROUTING -o ethX -j MASQUERADE，直接在物理网卡 ethX 上进行 NAT，所有从这个网卡出去的包都 NAT 成这个网卡的地址。
	- 设置 net.ipv4.ip_forward = 1，开启物理机的转发功能，直接做路由器，而不用单独的路由器

### 隔离

- 安全隔离 两个虚拟机可能属于两个用户，那怎么保证一个用户的数据不被另一个用户窃听？
- 流量隔离 两个虚拟机，如果有一个疯狂下片，会不会导致另外一个上不了网？
- 一台机器上虚拟机不属于同一个用户
	- brctl 创建网桥支持 VLAN 功能，可以设置两个虚拟机 tag，在虚拟网桥上，两个虚拟机是不互通的
- 跨物理机互通，实现 VLAN 隔离
	- 由于 brctl 创建的网桥上面 tag 没办法在网桥之外范围内起作用，需要寻找其他的方式
	- 命令 vconfig 可以创建基于物理网卡 eth0 带 VLAN 的虚拟网卡
	- 从这个虚拟网卡出去的包，都带这个 VLAN，跨物理机的互通和隔离可以通过这个网卡来实现
	- 为每个用户分配不同 VLAN，例如有一个用户 VLAN 10，一个用户 VLAN 20。在一台物理机上，基于物理网卡，为每个用户用 vconfig 创建一个带 VLAN 的网卡。不同用户使用不同的虚拟网桥，带 VLAN 的虚拟网卡也连接到虚拟网桥上。
	- 同主机不同的用户由于网桥不通，不能相互通信，一旦出了网桥，由于 VLAN 不同，也不会将包转发到另一个网桥上。
	- 出了物理机，也是带着 VLAN ID 的。只要物理交换机支持 VLAN 的，到达另一台物理机的时候，VLAN ID 依然在，它只会将包转发给相同 VLAN 的网卡和网桥，所以跨物理机，不同的 VLAN 也不会相互通信。

![[net_isolation_vm_diff_psy.png]]

### 软件定义网络 SDN

- 控制与转发分离
	- 转发平面 一个个虚拟或者物理网络设备
	- 控制平面 统一的控制中心。原来是一起的，物业管理员要从监控室出来，到路上去管理设备，现在是分离的，路就是走人的，控制都在监控室。
- 控制平面与转发平面之间开放接口
	- 北向接口 控制器向上提供接口，被应用层调用
	- 南向接口 控制器向下调用接口，来控制网络设备
- 逻辑上集中控制
	- 逻辑上集中的控制平面可以控制多个转发面设备，也就是控制整个物理网络，因而可以获得全局的网络状态视图，并根据该全局网络状态视图实现对网络的优化控制，就像物业管理员在监控室能够看到整个小区的情况，并根据情况优化出入方案。
- 最大意义在于让网络功能可编程

#### OpenFlow 和 OpenvSwitch

- OpenFlow 是 SDN 控制器和网络设备之间互通的南向接口协议
- OpenvSwitch 用于创建软件的虚拟交换机,支持 OpenFlow 协议的，当然也有一些硬件交换机也支持 OpenFlow 协议。都可以被统一的 SDN 控制器管理，从而实现物理机和虚拟机的网络连通。
- SDN 控制器通过 OpenFlow 协议控制网络
	- 有一个流表规则，任何通过这个交换机的包，都会经过这些规则进行处理，从而接收、转发、放弃。
	- 一个个表格串起来，每个表格好多行，每行都是一条规则。每条规则都有优先级，先看高优先级的规则，再看低优先级的规则。
	- 对于每一条规则，要看是否满足匹配条件。条件：从哪个端口进来的，网络包头里面有什么等等
	- 满足条件的网络包，就要执行一个动作，对这个网络包进行处理。
	- 可以修改包头里内容，可以跳到任何一个表格，可以转发到某个网口出去，也可以丢弃。
- 能力
	- 物理层
		- 匹配规则 从哪个口进来
		- 执行动作 从哪个口出去
	- MAC 层
		- 匹配规则
			- 源 MAC 地址是多少？（dl_src）
			- 目标 MAC 是多少？（dl_dst）
			- 所属 vlan 是多少？（dl_vlan）；
		- 执行动作
			- 修改源 MAC（mod_dl_src）
			- 修改目标 MAC（mod_dl_dst）
			- 修改 VLAN（mod_vlan_vid）
			- 删除 VLAN（strip_vlan）
			- MAC 地址学习（learn）
	- 网络层
		- 匹配规则
			- 源 IP 地址是多少？(nw_src)
			- 目标 IP 是多少？（nw_dst）
		- 执行动作
			- 修改源 IP 地址（mod_nw_src）
			- 修改目标 IP 地址（mod_nw_dst）
	- 传输层
		- 匹配规则
			- 源端口是多少？（tp_src）
			- 目标端口是多少？（tp_dst）
		- 执行动作
			- 修改源端口（mod_tp_src）
			- 修改目标端口（mod_tp_dst）
- 架构
	- 在用户态有两个重要进程，也有两个重要的命令行工具
		- OVSDB 进程 ovs-vsctl 命令行会和这个进程通信，去创建虚拟交换机，创建端口，将端口添加到虚拟交换机上，OVSDB 会将这些拓扑信息保存在一个本地的文件中
		- vswitchd 进程 ovs-ofctl 命令行会和这个进程通信，去下发流表规则，规则里面会规定如何对网络包进行处理，vswitchd 会将流表放在用户态 Flow Table 中。
	- 在内核态，有内核模块 OpenvSwitch.ko，对应图中 Datapath 部分。会在网卡上注册一个函数，每当有网络包到达网卡的时候，这个函数就会被调用。
		- 函数里面，会拿到网络包，将各个层次的重要信息拿出来
			- 在物理层，会拿到 in_port，即包是从哪个网口进来的
			- 在 MAC 层，会拿到源和目的 MAC 地址
			- 在 IP 层，会拿到源和目的 IP 地址
			- 在传输层，会拿到源和目的端口号
		- 内核态 Flow Table 内核态模块在内核态的流表中匹配规则，如果匹配上了，就执行相应的操作，比如修改包，或者转发，或者放弃。如果内核没有匹配上，这个时候就需要进入用户态，用户态和内核态之间通过 Linux 的一个机制叫 Netlink，来进行相互通信。
		- 内核通过 upcall，告知用户态进程 vswitchd，在用户态的 Flow Table 里面去匹配规则，里面规则是全量流表规则，而内核态的 Flow Table 只是为了做快速处理，保留了部分规则，内核里面的规则过一段时间就会过期。
		- 当在用户态匹配到了流表规则之后，就在用户态执行操作，同时将这个匹配成功的流表通过 reinject 下发到内核，从而接下来的包都能在内核找到这个规则，来进行转发
		- 调用 openflow 协议的，是本地的命令行工具。当然也可以是远程的 SDN 控制器来进行控制，一个重要的 SDN 控制器是 OpenDaylight。
- 在云计算中使用
- 通过 VIP 可以通过流表在不同的机器之间实现复杂均衡
	-  负载均衡服务器和业务服务器绑定一个虚拟IP(VIP)，通过MAC地址实现多个机器的负载均衡
- 在物理网络里面使用 VLAN，数目还是不够
	- VXLAN解决Vlan最多只能分配4096个的终端的问题。
	- VXLAN通过在第４层UDP帧中封装２层Vlan，完成Vllan扩容 4096x4096
			
![[OpenvSwitch_arch.png|OpenFlow 结构]]
![[OpenFlowRule.png]]
![[OpenvSwitchArch.png]]

#### 实现 VLAN 功能

- access port
	- tag 就是一个 VLAN ID
	- 从这个端口进来包都会被打上这个 tag
	- 网络包本身带有某个 VLAN ID 并且等于这个 tag，则这个包就会从这个 port 发出去,发出的包就会把 VLAN ID 去掉
- trunk port
	- 不配置任何 tag ，配置 trunks 参数
	- trunks 为空，则所有 VLAN 都 trunk
		- 本身带什么 VLAN ID，还是让携带着这个 VLAN ID
		- 如果没有设置 VLAN，就属于 VLAN 0，全部允许通过
	- 如果 trunks 不为空，则仅仅允许带着这些 VLAN ID 的包通过。
- 测试
	- 收到包和 ping 不同不矛盾
		- 要想 ping 的通，需要发送 ICMP 包，并且收到回复，而仅仅收到包，则不需要回复。
	- 从 192.168.100.102 来 ping 192.168.100.103，用 tcpdump 抓包
		- 192.168.100.102 和 first_br 都配置 tag103，都属于同一个 VLAN 103 的，first_if 能够收到包
		- 根据 access port 规则，从 first_br 出来包头是没有带 VLAN ID 的。
		- second_br 是 trunk port，所有 VLAN 都会放行，因而 second_if 能收到包的，根据 trunk port 规则，出来包的包头里面带有 VLAN ID 的。
		- third_br 配置允许 VLAN 101 和 102 通过，不允许 103 通过，third_if 收不到包的。
	- 从 192.168.100.100 来 ping 192.168.100.105: ping 不通，但是能够收到包
		- 因为 192.168.100.100 是配置 VLAN 101 的，因为 second_br 是配置了 trunk 的，是全部放行的，所以说 second_if 是可以收到包的,而且看到包头里面是带 VLAN 101 的。
		- third_br 配置可以放行 VLAN 101 和 102，所以说 third_if 是可以收到包,包头里面也是带 VLAN I101 。ping 不通，因为从 third_br 出来的包是带 VLAN 的，而 third_if 本身不属于某个 VLAN
	- 从 192.168.100.101 来 ping 192.168.100.104
		- 192.168.100.101 属于 VLAN 102 的， 因而 second_if 和 third_if 都因为配置了 trunk，是都可以收到包的。
		- first_br 属于 VLAN 103 的，不属于 VLAN 102，所以 first_if 是收不到包的。
		- second_br 能够收到包，并且包头里面是带 VLAN ID 102 的。
		- third_if 能收到包，并且包头里面也是带 VLAN ID 102 的。

![[vlan.png]]

```sh
ovs-vsctl add-br br0

# 实现 VLAN
ovs-vsctl add-port br0 first_br
ovs-vsctl add-port br0 second_br
ovs-vsctl add-port br0 third_br

ovs-vsctl set Port vnet0 tag=101
ovs-vsctl set Port vnet1 tag=102
ovs-vsctl set Port vnet2 tag=103

ovs-vsctl set Port first_br tag=103
ovs-vsctl clear Port second_br tag
ovs-vsctl set Port third_br trunks=101,102

# 禁止 MAC 地址学习
ovs-vsctl set bridge br0 flood-vlans=101,102,103
```

#### 模拟网卡绑定，连接交换机

- bond_mode
	- active-backup(default)：一个连接是 active，其他的是 backup，只有当 active 失效的时候，backup 才顶上
	- balance-slb：流量按照源 MAC 和 output VLAN 进行负载均衡
	- balance-tcp：必须在支持 LACP 协议的情况下才可以，可根据 L2、L3、L4 进行负载均衡（L2、L3、L4 指的是网络协议 2、3、4 层）
- 测试
	- 默认 active-backup 
		- 从 192.168.100.100 来 ping 192.168.100.102，从 192.168.100.101 来 ping 192.168.100.103 的时候，从 tcpdump 可以看到所有的包都是从 first_if 这条路通过。
		- 把 first_if 网卡设成 down 模式，则包走向会改变，发现 second_if 这条路开始有流量了，对于 192.168.100.100 和 192.168.100.101 从应用层来讲，感觉似乎没有受到影响。
	- 把 bond_mode 改为 balance-slb
		- 在 192.168.100.100 来 ping 192.168.100.102，同时也在 192.168.100.101 来 ping 192.168.100.103，包已经被分流了

![[openvSwitchCard.png]]

```sh
ovs-vsctl add-bond br0 bond0 first_br second_br
ovs-vsctl add-bond br1 bond1 first_if second_if
ovs-vsctl set Port bond0 lacp=active
ovs-vsctl set Port bond1 lacp=active

ovs-vsctl set Port bond0 bond_mode=balance-slb
ovs-vsctl set Port bond1 bond_mode=balance-slb
```

```sh
ovs-vsctl set port first_br qos=@newqos -- \
--id=@newqos create qos type=linux-htb other-config:max-rate=10000000 queues=0=@q0,1=@q1,2=@q2 -- \
--id=@q0 create queue other-config:min-rate=3000000 other-config:max-rate=10000000 -- \
--id=@q1 create queue other-config:min-rate=1000000 other-config:max-rate=10000000 -- \
--id=@q2 create queue other-config:min-rate=6000000 other-config:max-rate=10000000
```

### 安全

- 所有虚拟机共享一个机房网和公网 IP 地址，从外网网口出去的，都转换成为这个 IP 地址
	- Netfilter 连接跟踪（conntrack）功能
	- 对于 TCP 协议来讲，上来先建立一个连接，用“源 / 目的 IP+ 源 / 目的端口”唯一标识一条连接，连接会放在 conntrack 表里面。
	- 这台机器去请求网站的，源地址已经 Snat 成公网 IP 地址，但是 conntrack 表里面还是有这个连接的记录的。
	- 网站返回数据，会找到记录，从而找到正确的私网 IP 地址。

```sh
# 源地址转换 (Snat)
iptables -t nat -A -s 私网 IP -j Snat --to-source 外网 

# IP目的地址转换 (Dnat)
iptables -t nat -A -PREROUTING -d 外网 IP -j Dnat --to-destination 私网 IP
```

### QoS Quality of Service

- 通过 TC 控制网络的 QoS，就是通过队列的方式。
- 无类别排队规则 Classless Queuing Disciplines
	- pfifo_fast 分三个先入先出队列，称为三个 Band。
	- 根据网络包里面 TOS，看这个包到底应该进入哪个队列。TOS 总共四位，每一位表示的意思不同，总共十六种类型。
	-  `tc qdisc show dev eth0` 输出结果 priomap，也是十六个数字。在 0 到 2 之间，和 TOS 的十六种类型对应起来，表示不同的 TOS 对应的不同的队列。其中 Band 0 优先级最高，发送完毕后才轮到 Band 1 发送，最后才是 Band 2
-  随机公平队列 Stochastic Fair Queuing
	-  会建立很多的 FIFO 的队列，TCP Session 会计算 hash 值，通过 hash 值分配到某个队列。
	-  在队列另一端，网络包会通过轮询策略从各个队列中取出发送。这样不会有一个 Session 占据所有的流量。
	-  如果两个 Session 的 hash 是一样的，会共享一个队列，也有可能互相影响。hash 函数会经常改变，从而 session 不会总是相互影响。
-  令牌桶规则 TBF Token Bucket Filte
	-  所有的网络包排成队列进行发送，但不是到了队头就能发送，而是需要拿到令牌才能发送。
	-  令牌根据设定的速度生成，所以即便队列很长，也是按照一定的速度进行发送的。
	-  当没有包在队列中的时候，令牌还是以既定的速度生成，但是不是无限累积的，而是放满了桶为止。设置桶的大小为了避免下面的情况：当长时间没有网络包发送的时候，积累了大量的令牌，突然来了大量的网络包，每个都能得到令牌，造成瞬间流量大增。
-  分层令牌桶规则 HTB Hierarchical Token Bucket
	-  HTB 往往是一棵树，通过 TC 构建一棵 HTB 树
		-  对于网卡，需要规定发送速度。一般有两个速度可以配置，一个是 rate，表示一般情况下的速度；一个是 ceil，表示最高情况下的速度
		-  特性 同一个 root class 下子类可以相互借流量，如果不直接在队列规则下面创建一个 root class，而是直接创建三个 class，它们之间是不能相互借流量的。借流量策略，可以使得当前不使用这个分支流量的时候，可以借给另一个分支，从而不浪费带宽，使带宽发挥最大的作用。
		-  使用 TC 可以为某个网卡 eth0 创建一个 HTB 的队列规则，需要付给它一个句柄为（1:）,对于根节点来讲，两个速度是一样的，是创建一个 root class，速度为（rate=100kbps，ceil=100kbps）
		-  根节点，接下来会有分支。例如图中有三个分支，句柄分别为（:10）、（:11）、（:12）。最后参数 default 12，表示默认发送给 1:12，也即发送给第三个分支.每个子 class 统一有两个速度。三个分支分别为（rate=30kbps，ceil=100kbps）、（rate=10kbps，ceil=100kbps）、（rate=60kbps，ceil=100kbps）
		-  创建叶子队列规则，分别为 fifo 和 sfq
		-  通过 TC 设定发送规则
- 控制
	- 进入流量，可以设置策略 Ingress policy
	- 发出流量，可以设置 QoS 规则 Egress shaping，支持 HTB
		- 创建一个 QoS 规则，对应三个 Queue。min-rate 就是上面 rate，max-rate 就是上面 ceil
		- 通过交换机的网络包，要通过流表规则，匹配后进入不同的队列。然后就可以添加流表规则 Flow(first_br 是 br0 上的 port 5)

![[pfifo.png]]
![[htb.png]]

```sh
tc qdisc add dev eth0 root handle 1: htb default 12
tc class add dev eth0 parent 1: classid 1:1 htb rate 100kbps ceil 100kbps

tc class add dev eth0 parent 1:1 classid 1:10 htb rate 30kbps ceil 100kbps
tc class add dev eth0 parent 1:1 classid 1:11 htb rate 10kbps ceil 100kbps
tc class add dev eth0 parent 1:1 classid 1:12 htb rate 60kbps ceil 100kbps

tc qdisc add dev eth0 parent 1:10 handle 20: pfifo limit 5
tc qdisc add dev eth0 parent 1:11 handle 30: pfifo limit 5
tc qdisc add dev eth0 parent 1:12 handle 40: sfq perturb 10

tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 match ip dport 80 0xffff flowid 1:10
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 flowid 1:11

ovs-vsctl set Interface tap0 ingress_policing_rate=100000
ovs-vsctl set Interface tap0 ingress_policing_burst=10000

ovs-vsctl set port first_br qos=@newqos -- --id=@newqos \
create qos type=linux-htb other-config:max-rate=10000000 queues=0=@q0,1=@q1,2=@q2 --\
--id=@q0 create queue other-config:min-rate=3000000 other-config:max-rate=10000000 -- \
--id=@q1 create queue other-config:min-rate=1000000 other-config:max-rate=10000000 -- \
--id=@q2 create queue other-config:min-rate=6000000 other-config:max-rate=10000000

ovs-ofctl add-flow br0 "in_port=6 nw_src=192.168.100.100 actions=enqueue:5:0"
ovs-ofctl add-flow br0 "in_port=7 nw_src=192.168.100.101 actions=enqueue:5:1"
ovs-ofctl add-flow br0 "in_port=8 nw_src=192.168.100.102 actions=enqueue:5:2"
```

RPC 调用，应该用二进制还是文本类？其实文本的最大问题是，占用字节数目比较多。比如数字 123，其实本来二进制 8 位就够了，但是如果变成文本，就成了字符串 123。如果是 UTF-8 编码的话，就是三个字节；如果是 UTF-16，就是六个字节。同样的信息，要多费好多的空间，传输起来也更加占带宽，时延也高。

### 云中网络的隔离

- Underlay 网络 底层物理网络设备组成的网络
- Overlay 网络 用于虚拟机和云中的这些技术组成的网络，一种基于物理网络的虚拟化网络实现。

#### GRE Generic Routing Encapsulation

- 一种 IP-over-IP 隧道技术。将 IP 包封装在 GRE 包里，外面加上 IP 头，在隧道的一端封装数据包，并在通路上进行传输，到另外一端的时候解封装
- 帧结构
	- key 一个 32 位的字段，里面存放的往往就是用于区分用户的 Tunnel ID
- NVGRE 专门用于网络虚拟化的 GRE 包头格式
	- 网络 ID 号 24 位
- GRE 隧道传输过程。这里面有两个网段、两个路由器，中间要通过 GRE 隧道进行通信。当隧道建立之后，会多出两个 Tunnel 端口，用于封包、解封包。
	- 主机 A 在左边的网络，IP 地址为 192.168.1.102，要访问主机 B，主机 B 在右边的网络，IP 地址为 192.168.2.115。于是发送一个包，源地址为 192.168.1.102，目标地址为 192.168.2.115。因为要跨网段访问，于是根据默认 default 路由表规则，要发给默认的网关 192.168.1.1，也即左边的路由器。
	- 根据路由表，从左边的路由器，去 192.168.2.0/24 这个网段，应该走一条 GRE 的隧道，从隧道一端的网卡 Tunnel0 进入隧道。
	- 在 Tunnel 隧道端点进行包的封装，在内部的 IP 头之外加上 GRE 头。对于 NVGRE 来讲，是在 MAC 头之外加上 GRE 头，然后加上外部的 IP 地址，也即路由器的外网 IP 地址。源 IP 地址为 172.17.10.10，目标 IP 地址为 172.16.11.10，然后从 E1 的物理网卡发送到公共网络里。
	- 在公共网络里面，沿着路由器一跳一跳地走，全部都按照外部的公网 IP 地址进行。当网络包到达对端路由器的时候，也要到达对端的 Tunnel0，然后开始解封装，将外层的 IP 头取下来，然后根据里面的网络包，根据路由表，从 E3 口转发出去到达服务器 B。
- 不足
	- Tunnel 数量问题。GRE 是一种点对点隧道，如果有三个网络，就需要在每两个网络之间建立一个隧道。如果网络数目增多，这样隧道的数目会呈指数性增长
	- 不支持组播，因此一个网络中的一个虚机发出一个广播帧后，GRE 会将其广播到所有与该节点有隧道连接的节点。
	- 有很多防火墙和三层网络设备无法解析 GRE，因此它们无法对 GRE 封装包做合适地过滤和负载均衡。

![[gre_struct.png]]
![[gre_flow.png]]

#### VXLAN

- VXLAN 则是从二层外面就套了一个 VXLAN 的头，包含 VXLAN ID 为 24 位。在 VXLAN 头外面封装 UDP、IP，以及外层的 MAC 头。
- VTEP（VXLAN Tunnel Endpoint）  VXLAN 作为扩展性协议，实现对 VXLAN 的包进行封装和解封装功能
	- VTEP 相当于虚拟机网络的管家。每台物理机上都可以有一个 VTEP。
	- 每个虚拟机启动的时候，都需要向这个 VTEP 管家注册，每个 VTEP 都知道自己上面注册了多少个虚拟机。
	- 当虚拟机要跨 VTEP 进行通信的时候，需要通过 VTEP 代理进行，由 VTEP 进行包的封装和解封装。
- VXLAN 不是点对点的，支持通过组播的来定位目标机器的
	- 当一个 VTEP 启动的时候，需要通过 IGMP 协议。加入一个组播组. 而当每个物理机上的虚拟机启动之后，VTEP 就知道，有一个新的 VM 上线了，它归我管。

#### 云平台中使用 overlay

- 多一个 br1 虚拟交换机呢？主要通过 br1 这一层将虚拟机之间的互联和物理机机之间的互联分成两层来设计，中间隧道可以有各种挖法，GRE、VXLAN 都可以
- 所有的 Flow Table 规则都设置在 br1 上，每个 br1 都有三个网卡，网卡 1 对内的，网卡 2 和 3 是对外的。
- Table 0 所有流量入口，所有进入 br1 流量，分为两种流量，一个是进入物理机的流量，一个是从物理机发出的流量。
- Table 1 用于处理所有出去网络包，分为两种情况，一种是单播，一种是多播。
- Table 2 紧接着 Table1 的，如果既不是单播，也不是多播，就默认丢弃
- Table 3 用于处理所有进来网络包，需要将隧道 Tunnel ID 转换为 VLAN ID。
- 对于进来包，Table 10 会进行 MAC 地址学习。学习结果放在 Table 20(MAC learning table) 里面。二层交换机应该做的事情，学习完了之后，再从 port 1 发出去
	- NXM_OF_VLAN_TCI 是 VLAN tag。在 MAC learning table 中，每一个 entry 都仅仅是针对某一个 VLAN 来说的，不同 VLAN 的 learning table 是分开的。在学习结果的 entry 中，会标出这个 entry 是针对哪个 VLAN 的
	- NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[]表示，当前包里面的 MAC Source Address 会被放在学习结果的 entry 里的 dl_dst 里。这是因为每个交换机都是通过进入的网络包来学习的。某个 MAC 从某个 port 进来，交换机就应该记住，以后发往这个 MAC 的包都要从这个 port 出去，因而源 MAC 地址就被放在了目标 MAC 地址里面，因为这是为了发送才这么做的。
	- load:0->NXM_OF_VLAN_TCI[]是说，在 Table20 中，将包从物理机发送出去的时候，VLAN tag 设为 0，所以学习完了之后，Table 20 中会有 actions=strip_vlan。
	- load:NXM_NX_TUN_ID[]->NXM_NX_TUN_ID[]的意思是，在 Table 20 中，将包从物理机发出去的时候，设置 Tunnel ID，进来的时候是多少，发送的时候就是多少，所以学习完了之后，Table 20 中会有 set_tunnel。
	- output:NXM_OF_IN_PORT[]是发送给哪个 port。例如是从 port 2 进来的，那学习完了之后，Table 20 中会有 output:2。
- Table 20 是 MAC Address Learning Table。如果不为空，就按照规则处理；如果为空，就说明没有进行过 MAC 地址学习，只好进行广播了，因而要交给 Table 21 处理。
- Table 21 用于处理多播的包
	- 如果匹配不上 VLAN ID，就默认丢弃。
	- 如果匹配上 VLAN ID，将 VLAN ID 转换为 Tunnel ID，从两个网卡 port 2 和 port 3 都发出去，进行多播。

![[overlay_archtect.png]]
![[overlay_flow_table.png]]

```sh
# 从 port 1 进来的，都是发出去的流量，全部由 Table 1 处理。
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1 in_port=1 actions=resubmit(,1)"

# 从 port 2、3 进来的，都是进入物理机的流量，全部由 Table 3 处理
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1 in_port=2 actions=resubmit(,3)"
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1 in_port=3 actions=resubmit(,3)"
# 如果都没匹配上，就默认丢弃。
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=0 actions=drop"

# 对于单播，由 Table 20 处理。
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1 table=1 dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20)"
# 对于多播，由 Table 21 处理
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1 table=1 dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,21)"

# Table 2 是紧接着 Table1 的，如果既不是单播，也不是多播，就默认丢弃。
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=0 table=2 actions=drop"

# 如果匹配不上 Tunnel ID，就默认丢弃。
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=0 table=3 actions=drop"

# 如果匹配上了 Tunnel ID，就转换为相应的 VLAN ID，然后跳到 Table 10。
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1 table=3 tun_id=0x1 actions=mod_vlan_vid:1,resubmit(,10)"
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1 table=3 tun_id=0x2 actions=mod_vlan_vid:2,resubmit(,10)"

ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1 table=10  actions=learn(table=20,priority=1,hard_timeout=300,NXM_OF_VLAN_TCI[0..11],NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],load:0->NXM_OF_VLAN_TCI[],load:NXM_NX_TUN_ID[]->NXM_NX_TUN_ID[],output:NXM_OF_IN_PORT[]),output:1"

ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=0 table=20 actions=resubmit(,21)"

ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=0 table=21 actions=drop"
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1table=21dl_vlan=1 actions=strip_vlan,set_tunnel:0x1,output:2,output:3"
ovs-ofctl add-flow br1 "hard_timeout=0 idle_timeout=0 priority=1table=21dl_vlan=2 actions=strip_vlan,set_tunnel:0x2,output:2,output:3"
```

## [容器](../cncf/docker.md)

## 微服务

Amazon S3 在2008年7月就遇到过，单bit反转导致了一次严重线上事故，所以他们吸取教训加了 check sum。见<http://status.aws.amazon.com/s3-20080720.html>
