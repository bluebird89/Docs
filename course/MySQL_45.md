## [MySQL实战45讲](https://time.geekbang.org/column/intro/139) 林晓斌 网名丁奇，腾讯云数据库负责人

- 数据库是一个综合系统，其背后是发展了几十年的数据库理论。
- 同时，数据库系统也是一个应用系统，可能一个业务开发人员用了两三年 MySQL，还未必清楚那些自己一直在用的“最佳实践”为什么是最佳的。

### 基础

- 表逻辑结构 表 —> 段 —> 段中存在数据段(leaf node segment) ，索引段( Non-leaf node segment）
  - 数据段 主键索引数据
  - 索引段 二级索引数据
- 建立的每个索引都有要维护一个数据段,那么新插入一行值 ， 每个索引段都会维护这个值
- 树高取决于叶子树（数据行数）和“N叉树”的N。 而N是由页大小和索引大小决定的
- 查询数据大致流程细化
  - 通过执行器调用引擎到表里的数据段／索引段取数据
  - 数据是按照段->区->页维度去取 ， 取完后先放到数据缓冲池中
  - 再通过二分法查询叶结点的有序链表数组找到行数据返回给用户
  - 当数据量大的时候，会存在不同的区，取范围值的时候会到不同的区取页的数据返回用户
- Online DDL 过程
  - 拿MDL写锁
  - 降级成MDL读锁
  - 真正做DDL
  - 升级成MDL写锁
  - 释放MDL锁
  - 1、2、4、5如果没有锁冲突，执行时间非常短。第3步占用DDL绝大部分时间，这期间这个表可以正常读写数据，是因此称为“online ”
- 有两个“视图”的概念
	- 一个是 view。是一个用查询语句定义的虚拟表，调用时候执行查询语句并生成结果。
		- 创建视图语法 create view … ，查询方法与表一样
	- 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现
		- 没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”

#### 01 | 基础架构：一条SQL查询语句是如何执行的？

![[../_static/mysql_basic_archtect.png]]

- Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。
  - 不同的存储引擎共用一个 Server 层，也就是从连接器到执行器的部分
  - 连接器 `mysql -h$ip -P$port -u$user -p`
    - 连接到数据库上，接待的就是连接器
    - 负责跟客户端建立连接、获取权限、维持和管理连接
    - 连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。完成经典 TCP 握手后，连接器就要开始认证身份，用的是输入的用户名和密码
    - 如果用户名或密码不对，就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。
    - 如果用户名密码认证通过，连接器会到权限表里面查出拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。
      - 一个用户成功建立连接后，即使用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建连接才会使用新的权限设置。
    - 没有后续的动作，连接就处于空闲状态，可以在 `show processlist` 命令中看到
    - 客户端太长时间没动静，连接器就会自动将它断开。这个时间由参数 wait_timeout 控制，默认值 8 小时
    - 连接被断开之后，客户端再次发送请求，会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果要继续，就需要重连，然后再执行请求
    - 长连接 vs 短连接
      - 长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接
      - 短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个
      - 建立连接的过程通常是比较复杂的，建议在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。
      - 全部使用长连接后，会发现，有些时候 MySQL 占用内存涨得特别快，因为 MySQL 在执行过程中临时使用内存管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了
        - 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
        - 如果用 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，会将连接恢复到刚刚创建完时的状态。
  - 查询缓存
    - 之前执行过语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中,key 是查询的语句，value 是查询的结果
    - 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句
      - 如果查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端
      - 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中
    - 如果查询命中缓存，MySQL 不需要执行后面的复杂操作直接返回结果，效率会很高
    - 大多数情况下建议不要使用查询缓存
      - 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空
      - 对于更新压力大的数据库来说，查询缓存的命中率会非常低。
      - 除非业务就是有一张静态表，很长时间才会更新一次
    - 提供“按需使用”的方式,将参数 query_cache_type 设置成 DEMAND
      - 默认的 SQL 语句都不使用查询缓存
      - 确定要使用查询缓存 `select SQL_CACHE * from T where ID=10；`
    - 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了
  - 分析器
    - 词法分析 输入由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么
    - 语法分析 根据语法规则，判断输入 SQL 语句是否满足 MySQL 语法,如果语句不对，就会收到“You have an error in your SQL syntax”的错误提醒
      - 一般语法错误会提示第一个出现错误的位置，所以关注的是紧接“use near”的内容
    - 解析器处理语法和解析查询, 生成一课对应的解析树。
    - 预处理器进一步检查解析树的合法。比如: 数据表和数据列是否存在, 别名是否有歧义等。如果通过则生成新的解析树，再提交给优化器。
  - 优化器
    - 在表里面有多个索引的时候，决定使用哪个索引
    - 在一个语句有多表关联（join）的时候，决定各个表的连接顺序 `select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;`
      - 可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20
      - 可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10
    - 两种执行方法的逻辑结果是一样的，但是执行效率会有不同，而优化器的作用就是决定选择使用哪一个方案
  - 执行器
    - 开始执行的时候，要先判断一下对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误
    - 如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证,查询也会在优化器之前调用 precheck 验证权限
    - 有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口
    - `select * from T where ID=10;`
      - 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中
      - 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
      - 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端
    - 在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的
      - 有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的
- 存储引擎层 负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎
  - 现在最常用存储引擎 InnoDB，从 MySQL 5.5.5 版本开始成为默认存储引擎。

### 索引

- 索引的出现为了提高数据查询的效率，就像书的目录一样，对于数据库的表而言，索引其实就是它的“目录”

#### 04 | 深入浅出索引（上）

##### 索引模型

- 哈希表
  - 一种以键 - 值（key-value）存储数据的结构，只要输入待查找的键即 key，就可以找到其对应值即 Value
    - Value 不需要比对，可以使用复杂的数据结构，多存储内容
  - 思路 用一个哈希函数把 key 换算成一个确定位置，然后把 value 放在数组的这个位置
  - 不可避免 多个 key 值经过哈希函数的换算，会出现同一个值的情况
    - 一种方法 拉出一个链表，Value 结构中存在 key，用于比对
  - Key 值并不是递增的
  - 好处 增加新 Value 时速度会很快，只需要往后追加
  - 缺点 因为不是有序，做区间查询的速度很慢
  - 适用于只有等值查询场景，比如 Memcached 及其他一些 NoSQL 引擎
- 有序数组在等值查询和范围查询场景中的性能就都非常优秀
  - 假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的
  - 等值查询 要查 ID_card_n2 对应名字，用二分法就可以快速得到，时间复杂度 O(log(N))
  - 范围查询 先用二分法找到 ID_card_X（如果不存在 ID_card_X，就找到大于 ID_card_X 的第一个 User），然后向右遍历，直到查到第一个大于 ID_card_Y 的身份证号，退出循环
  - 需要更新数据时就麻烦了，往中间插入一个记录就必须得挪动后面所有的记录，成本太高
  - 只适用于静态存储引
- 搜索树
  - 二叉搜索树特点：父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。
    - 要查 ID_card_n2 的话，按照图中搜索顺序 UserA -> UserC -> UserF -> User2 路径得到。时间复杂度 O(log(N))
    - 为维持 O(log(N)) 的查询复杂度，需要保持这棵树是平衡二叉树。为了做这个保证，更新时间复杂度也是 O(log(N))
  - 多叉树 每个节点有多个儿子，儿子之间的大小保证从左到右递增
    - 由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了
  - 二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。原因 索引不止存在内存中，还要写到磁盘上
    - 一棵 100 万节点的平衡二叉树，树高 20
    - 一次查询可能需要访问 20 个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要 10 ms 左右的寻址时间。也就是说，对于一个 100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要 20 个 10 ms 的时间，这个查询可真够慢的
    - 为了让一个查询尽量少地读磁盘，必须让查询过程访问尽量少数据块。就不应该使用二叉树，而是要使用“N 叉”树。“N”取决于数据块的大小
    - 以 InnoDB 的一个整数字段索引为例，N 差不多是 1200
      - 树高是 4 时候，可以存 1200 的 3 次方个值，已经 17 亿了
      - 考虑到树根数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘
      - 其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了
- 不管是哈希还是有序数组，或者 N 叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库技术发展到今天，跳表、LSM 树等数据结构也被用于引擎设计中
- 数据库底层存储的核心是基于这些数据模型的。每碰到一个新数据库，需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景

##### InnoDB 实现

- 索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于 InnoDB 存储引擎在 MySQL 数据库中使用最为广泛，以 InnoDB 为例
- 在 InnoDB 中，表都是根据主键顺序以索引形式存放，这种存储方式的表称为索引组织表。InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中
  - B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数
- 每一个索引在 InnoDB 里面对应一棵 B+ 树
- 索引类型
  - 有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引，表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)
  - 主键索引 叶子节点存的是整行数据
    - 在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）
  - 非主键索引 叶子节点内容是主键的值
    - 在 InnoDB 里，非主键索引也被称为二级索引（secondary index）
- 查询 基于主键索引和普通索引区别
  - 主键查询方式 语句 `select * from T where ID=500`，只需要搜索 ID 这棵 B+ 树
  - 普通索引查询方式 语句`  select * from T where k=5 `，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表
  - 基于非主键索引的查询需要多扫描一棵索引树,在应用中应该尽量使用主键查询
- 维护
  - B+ 树为维护索引有序性，在插入新值时需要做必要的维护。以下图为例
    - 插入新行 ID 值为 700，只需要在 R5 记录后面插入一个新记录
    - 新插入 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置
      - 页分裂 更糟情况 如果 R5 所在数据页已经满了，根据 B+ 树算法，需要申请一个新数据页，然后挪动部分数据过去
        - 在这种情况下，性能自然会受影响
        - 影响数据页利用率。原本放在一个页数据，现在分到两个页中，整体空间利用率降低大约 50%
  - 页合并
    - 当相邻两个页由于删除数据，利用率很低之后，会将数据页做合并
    - 合并过程，可以认为是分裂过程的逆过程
- 自增主键
  - 自增主键 指自增列上定义的主键，在建表语句中定义 `bigint unsigned NOT NULL PRIMARY KEY AUTO_INCREMENT` 插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值
  - 自增主键的插入数据模式，正符合递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂
  - 有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高
  - 从存储空间的角度来看,表中确实有一个唯一字段，比如字符串类型的身份证号，用身份证号做主键，还是用自增字段做主键呢
    - 由于每个非主键索引的叶子节点上都是主键的值
    - 如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节
    - 如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节
  - 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小
  - 从性能和存储空间方面考量，自增主键往往是更合理的选择
  - 适合用业务字段直接做主键场景
    - KV 场景 只有一个索引,该索引必须是唯一索引
    - 根据"尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树
- 重建主键索引
  - 删除表的部分记录,但是它的索引还在, 并未释放
  - 重建索引 会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空
  - 不论是删除主键还是创建主键，都会将整个表重建
  - 整个数据库迁移，先dump出来再重建表（这个一般只适合离线的业务来做）
  - 用空的alter操作，比如`ALTER TABLE t1 ENGINE = InnoDB;` 会原地重建表结构（真的吗？）
  - 第三个是用 repaire table，不过这个是由存储引擎决定支不支持的（innodb就不行）

```sql
mysql> create table T(
id int primary key, 
k int not null, 
name varchar(16),
index (k))engine=InnoDB;
```

![[../_static/innodb_index_example_struct.png]]

#### 05 | 深入浅出索引（下）

- 覆盖索引 `select ID from T where k between 3 and 5`
  - 非主键索引树“覆盖了”查询需求中的字段，可以直接提供查询结果，不需要回表
  - 可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段
  - 索引 k 已经“覆盖了”查询需求
  - 在引擎内部使用覆盖索引在索引 k 上读了三个记录，R3~R5（对应索引 k 上的记录项），但是对于 MySQL 的 Server 层来说，就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2
- 冗余索引 在一个市民信息表上，是否有必要将身份证号和名字建立联合索引
  - 如果现在要根据市民的身份证号查询他的姓名高频请，这个联合索引就有意义
  - 索引字段的维护总是有代价的。在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作
- 前缀索引
  - 最左前缀原则  B+ 树这种索引结构，可以利用索引“最左前缀”，来定位记录
    - 只要满足最左前缀，就可以利用索引来加速检索,可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符
    - 用（name，age）联合索引,索引项是**按照索引定义里面出现的字段顺序排序**
    - 查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果
    - 查的是所有名字第一个字是“张”的人 "where name like ‘张 %’",也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止
  - 如何安排索引内的字段顺序 评估标准 索引的复用能力
    - 如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的
    - 有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候不得不维护另外一个索引，需要同时维护 (a,b)、(b) 这两个索引.这时候，要考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，建议创建一个（name,age) 的联合索引和一个 (age) 的单字段索引
- 索引下推
  - 以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩  `select * from tuser where name like '张%' and age=10 and ismale=1;`
  - 只能用 “张”，找到第一个满足条件的记录 ID3。当然，这还不错，总比全表扫描要好
  - 然后是判断其他条件是否满足
    - MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值
    - MySQL 5.6 引入索引下推优化（index condition pushdown)， 在索引遍历过程中，对索引中包含字段先做判断，直接过滤掉不满足条件的记录，减少回表次数
- 主键索引也是可以使用多个字段
  - 为什么要创建“ca”“cb”这两个索引,因为业务里面有这样的两种语句
  - 为了这两个查询模式，这两个索引是否都是必须的？为什么呢
    - 主键 a，b 聚簇索引组织顺序相当于 order by a,b ，先按 a 排序，再按 b 排序，c 无序
    - InnoDB 排序 会把主键字段放到索引定义字段后面，当然同时也会去重
    - 当主键是(a,b)的时候，定义为c的索引，实际上是（c,a,b);定义为(c,a)的索引，实际上是(c,a,b),定义为(c,b）的索引，实际上是（c,b,a)
    - B+树的叶子节点指向一个InnoDB数据页，数据页里边会有多行数据，多行数据是以有序数组的形式保存的。c的二级索引里，（a,b)就是所谓的行数据。在c这个二级索引的B+树上的叶子节点上，（a，b）是以有序数组的形式排好序的。所以c二级索引和ca联合索引一模一样的
    - 索引 ca 组织 先按 c 排序，再按 a 排序，同时记录`主键部分b`
    - 索引 cb 组织 先按 c 排序，在按 b 排序，同时记录`主键部分a`
    - ca 可以去掉，cb 需要保留

![[../_static/b+tree_index_strcut.png]]

```sql
CREATE TABLE `tuser` (
  `id` int(11) NOT NULL,
  `id_card` varchar(32) DEFAULT NULL,
  `name` varchar(32) DEFAULT NULL,
  `age` int(11) DEFAULT NULL,
  `ismale` tinyint(1) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `id_card` (`id_card`),
  KEY `name_age` (`name`,`age`)
) ENGINE=InnoDB


CREATE TABLE `geek` (
  `a` int(11) NOT NULL,
  `b` int(11) NOT NULL,
  `c` int(11) NOT NULL,
  `d` int(11) NOT NULL,
  PRIMARY KEY (`a`,`b`),
  KEY `c` (`c`),
  KEY `ca` (`c`,`a`),
  KEY `cb` (`c`,`b`)
) ENGINE=InnoDB;

select * from geek where c=N order by a limit 1;
select * from geek where c=N order by b limit 1;
```

- 09 | 普通索引和唯一索引，应该怎么选择？
- 10 | MySQL为什么有时候会选错索引？
- 11 | 怎么给字符串字段加索引？
- 15 | 答疑文章（一）：日志和索引相关问题
- 16 | “order by”是怎么工作的？
- 18 | 为什么这些SQL语句逻辑相同，性能却差异巨大？

### 事务

#### 03 | 事务隔离：为什么你改了我还看不见？

- 事务 要保证一组数据库操作，要么全部成功，要么全部失败。
- 在 MySQL 中，事务支持是在引擎层实现的
- 并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一

![[../_static/transaction_example.png]]

- 隔离级别
  - 隔离得越严实，效率就会越低
  - 读未提交（read uncommitted）一个事务还没提交时，做的变更就能被别的事务看到
  - 读提交（read committed）一个事务提交后，做的变更才会被其他事务看到
  - 可重复读（repeatable read）一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。未提交变更对其他事务也是不可见的
    - 管理个人银行账户表。一个表存了账户余额，一个表存了账单明细。到了月底你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。
    - 一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响校对结果。这时候使用“可重复读”隔离级别就很方便。
    - 事务启动时的视图可以认为是静态的，不受其他事务更新的影响。
  - 串行化（serializable ）对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突时候，后访问事务必须等前一个事务执行完成，才能继续执行
    - 在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行
- 实现
  - 每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值
    - 同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）
    - 回滚日志不能一直保留，在不需要的时候才删除
    - 系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除
    - 当系统里没有比这个回滚日志更早的 read-view 时
  - 数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准
  - “可重复读”隔离级别，视图在事务启动时创建，整个事务存在期间都用这个视图
    - 一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面会有类似下面的记录
    - 当前值 4，但在查询这条记录时，不同时刻启动的事务会有不同的 read-view。
    - 如图中看到的，在视图 A、B、C 里面，这一个记录值分别是 1、2、4
    - 同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）
    - 对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到
    - 即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的
  - “读提交”隔离级别，视图在每个 SQL 语句开始执行时创建
  - 注意
    - “读未提交”隔离级别下直接返回记录上最新值，没有视图概念
    - “串行化”隔离级别下直接用加锁的方式来避免并行访问
- Oracle 数据库默认隔离级别“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，记得将 MySQL 的隔离级别设置为“读提交”
  - 将启动参数 transaction-isolation 值设置成 READ-COMMITTED
- 建议尽量不要使用长事务
  - 查找持续时间超过 60s 事务 `select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60`
  - 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面可能用到的回滚记录都必须保留，会导致大量占用存储空间
  - MySQL 5.5 及以前版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库
  - 长事务还占用锁资源，也可能拖垮整个库
- 启动方式
  - 显式启动事务语句， begin 或 start transaction。配套提交语句 commit，回滚语句 rollback
    - 如果考虑多一次交互问题，可以使用commit work and chain语法
  - set autocommit=0，将线程自动提交关掉。意味着如果只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到主动执行 commit 或 rollback 语句，或者断开连接
- 建议使用 set autocommit=1, 通过显式语句方式来启动事务
  - 用 begin 显式启动事务，如果执行 commit 则提交事务。
  - 如果执行 commit work and chain，则是提交事务并自动启动下一个事务，省去了再次执行 begin 语句的开销。同时带来好处是从程序开发角度明确地知道每个语句是否处于事务中
- 如何避免长事务对业务的影响
  - 开发
    - 确认是否使用 set autocommit=0。这个工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，目标就是把它改成 1。
    - 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。
    - 业务连接数据库的时候，根据业务本身预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。
  - 从数据库端来看
    - 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 killPercona 的 pt-kill 这个工具不错，推荐使用
    - 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题
    - 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。 如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便

![[../_static/repeat_read_view.png]]

#### 08 | 事务到底是隔离的还是不隔离的？

- 可重复读隔离级别，事务 T 启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样
- 一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。
  - 问题，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢
- begin/start transaction 命令并不是一个事务起点，执行到它们之后第一个操作 InnoDB 表的语句，事务才真正启动。如果想要马上启动一个事务，使用 start transaction with consistent snapshot 命令

```sql

mysql> CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `k` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
insert into t(id, k) values(1,1),(2,2);
```

| transaction A                              | transaction B                                                     | transactionC                    |
| ------------------------------------------ | ----------------------------------------------------------------- | ------------------------------- |
| start transaction with consistent snapshot |                                                                   |                                 |
|                                            | start transaction with consistent snapshot                        |                                 |
|                                            |                                                                   | update t set k =k+1 where id =1 |
|                                            | update t set k =k+1 where id =1; select k from t where id =1; |                                 |
| select k from t where id =1;               |                                                                   |                                 |

- 事务 B 查到 k 值是 3，而事务 A 查到 k 值是 1
- “快照”在 MVCC 里是怎么工作的？
	- 在可重复读隔离级别下，事务在启动时“拍了个快照”。注意 快照是基于整库的
	- InnoDB 里面每个事务有一个唯一事务 ID transaction id, 在事务开始时向 InnoDB 事务系统申请的，是按申请顺序严格递增的
	- 每行数据都有多个版本,每次事务更新数据时，都会生成一个新数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id
	- 同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它
	- 数据表中一行记录，可能有多个版本 (row)，每个版本有自己的 row trx_id
	- 一个记录可以同时被多个事务连续更新，事务之间肯定有先后顺序
	- 多版本并不是物理上真实存在的，而是每次需要时根据当前版本和 undo log 计算出来的
- 可重复读定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见
	- 一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认
	- 如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本

一张表两个字段id, uname,id主键，uname普通索引
SELECT * FROM test_like WHERE uname LIKE 'j'/ 'j%' / '%j'/ '%j%'
模糊查询like后面四种写法都可以用到uname的普通索引

添加一个age字段
like后面的'%j'/ '%j%' 这两种情况用不到索引
把select * 改为 select id / select uname / select id,uname
like后面'j'/ 'j%' / '%j'/ '%j%' 这四种情况又都可以用到uname普通索引

建立uname,age的联合索引
模糊查询还是 LIKE 'j'/ 'j%' / '%j'/ '%j%'四种情况
其中select id / select uname / select id,uname
会用到uname的普通索引
select * 会用到uname,age的组合索引

#### 20 | 幻读是什么，幻读有什么问题？

### 锁

#### 06 | 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？

- 全局锁 对整个数据库实例加锁
  - MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)
  - 当需要让整个库处于只读状态时候，可以使用这个命令，之后其他线程以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句
  - 使用场景 做全库逻辑备份,就是把整库每个表都 select 出来存成文本
    - 不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的
  - 一致性读 官方自带逻辑备份工具 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的
    - 一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性
    - single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一
  - 为什么不使用 set global readonly=true 方式
    - 在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，不建议使用
    - 在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高
    - 在 slave 上 如果用户有超级权限的话 readonly 是失效的
- 表级锁
  - 表锁
    - `lock tables … read/write`
    - 用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放
    - 注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象
      - 在某个线程 A 中执行 `lock tables t1 read, t2 write;` 语句
      - 其他线程写 t1、读写 t2 的语句都会被阻塞
      - 同时，线程 A 在执行 unlock tables 之前，只能执行读 t1、读写 t2 操作
      - 连写 t1 都不允许
    - 一般是在数据库引擎不支持行锁的时候才会被用到
    - 对于 InnoDB 这种支持行锁引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大
    - 发现应用程序里有 lock tables 这样的语句
      - 系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎
      - 引擎升级但代码还没升级。业务开发就是把 lock tables 和 unlock tables 改成 begin 和 commit
  - 元数据锁（meta data lock，MDL)
    - 不需要显式使用，在访问一个表的时候会被自动加上
    - 作用 保证读写的正确性
    - 直到事务提交才释放
    - 5.5 版本中引入，当对一个表做增删改查操作，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁
      - 读锁之间不互斥，可以有多个线程同时对一张表增删改查
      - 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行
    - 给一个小表加个字段，导致整个库挂了
      - session A 先启动，这时候会对表 t 加一个 MDL 读锁
      - 由于 session B 需要的也是 MDL 读锁，因此可以正常执行
      - session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。
      - 如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞
      - 如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满
      - 事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放
      - 该怎么做
        - 比较理想机制，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。
      - MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法

```sql
ALTER TABLE tbl_name NOWAIT add column ...
ALTER TABLE tbl_name WAIT N add column ... 
```

- 用–single-transaction 方法在备库上执行逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢
  - 假设这个 DDL 是针对表 t1 的
  - 在备份开始的时候，为了确保 RR（可重复读）隔离级别，再设置一次 RR 隔离级别 (Q1)
  - 启动事务，这里用 WITH CONSISTENT SNAPSHOT 确保这个语句执行完就可以得到一个一致性视图（Q2)
  - 设置一个保存点（Q3）
  - show create 是为了拿到表结构 (Q4)，然后正式导数据 （Q5），回滚到 SAVEPOINT sp，在这里的作用是释放 t1 的 MDL 锁 （Q6）
  - 参考答案
    - 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构
    - 如果在“时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止
    - 如果在“时刻 2”和“时刻 3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成
    - 从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构

```sql
Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
Q2:START TRANSACTION  WITH CONSISTENT SNAPSHOT；
/* other tables */
Q3:SAVEPOINT sp;
/* 时刻 1 */
Q4:show create table `t1`;
/* 时刻 2 */
Q5:SELECT * FROM `t1`;
/* 时刻 3 */
Q6:ROLLBACK TO SAVEPOINT sp;
/* 时刻 4 */
/* other tables */
```

#### 07 | 行锁功过：怎么减少行锁对性能的影响？

- 行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一
- innodb行级锁是通过锁索引记录实现,如果update的列没建索引，即使只update一条记录也会锁定整张表
- 两阶段锁协议
  - 在 InnoDB 事务中，行锁是在需要时候才加上
  - 并不是不需要了就立刻释放，而是要等到事务结束时才释放
  - 如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放
- 死锁 当并发系统中不同线程出现循环资源依赖，涉及线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态
  - 一种策略 直接进入等待，直到超时，超时时间通过参数 innodb_lock_wait_timeout 来设置
    - 默认值 50s，对于在线服务来说，这个等待时间往往是无法接受的
    - 又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。
  - 另一种策略，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑
    - 要加锁访问的行上有锁，才要检测
    - 并不是每次死锁检测都都要扫所有事务,B在等A，D在等C，现在来了一个E，发现E需要等D，那么E就判断跟D、C是否会形成死锁，这个检测不用管B和A
- 正常情况下采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。
  - 每当一个事务被锁的时候，就要看看它所依赖线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁
  - 所有事务都要更新同一行场景，每个新来被堵住的线程，都要判断会不会由于自己的加入导致了死锁，时间复杂度 O(n) 操作
  - 假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，就会看到 CPU 利用率很高，但是每秒却执行不了几个事务
- 怎么解决由这种热点行更新导致的性能问题呢
  - 症结在于，死锁检测要耗费大量的 CPU 资源
  - 一种头痛医头方法 如果能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。
  - 控制并发度。根据上面分析发现如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题
    - 一个直接想法就是，在客户端做并发控制。但是，会很快发现这个方法不太可行，因为客户端很多。见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000
    - 要做在数据库服务端。如果有中间件，可以考虑在中间件实现；如果团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。
    - 基本思路 对于相同行更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了
  - 不能实现上面方案，从设计上优化
    - 通过将一行改成逻辑上的多行来减少锁冲突
    - 以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院账户总额等于这 10 个记录的值的总和。
    - 每次要给影院账户加金额的时候，随机选其中一条记录来加。每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。
    - 看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。
- 删除一个表里面的前 10000 行数据
  - 直接执行 delete from T limit 10000
  - 在一个连接中循环执行 20 次 delete from T limit 500
  - 在 20 个连接中同时执行 delete from T limit 500

![[../_static/line_lock_example.png]]

- 19 | 为什么我只查一行的语句，也执行这么慢？
- 20 | 幻读是什么，幻读有什么问题？
- 21 | 为什么我只改一行的语句，锁这么多？
- 30 | 答疑文章（二）：用动态的观点看加锁
- 40 | insert语句的锁为什么这么多？

### 日志与主备

#### 02 | 日志系统：一条SQL更新语句是如何执行的？

- 更新流程涉及两个重要的日志模块 redo log（重做日志）和 binlog（归档日志），redo log 和 binlog 在设计上有很多有意思的地方，这些设计思路也可以用到自己的程序里

- redo log 是 InnoDB 引擎特有日志，而 Server 层有自己的日志，称为 binlog（归档日志）
  - 最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档
  - InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统—— redo log 来实现 crash-safe 能力
  - 不同
    - redo log 是 InnoDB 引擎特有；binlog 是 MySQL 的 Server 层实现，所有引擎都可以使用
    - redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”
    - redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志

- redo log
  - 酒店掌柜有一个粉板，专门用来记录客人的赊账记录。如果赊账的人不多，可以把顾客名和账目写在板上
  - 赊账人多，粉板总会有记不下的时候，一定还有一个专门记录赊账的账本
    - 操作太麻烦
      - 得找到这个人的赊账总额那条记录,密密麻麻几十页，找到那个名字
      - 找到之后再拿出算盘计算，最后再将结果写回到账本上
      - 如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高
  - WAL Write-Ahead Logging
    - 粉板和账本配合的整个过程,先写粉板，等不忙的时候再写账本
    - 关键点 先写日志，再写磁盘
    - 当有一条记录需要更新，InnoDB 引擎就会先把记录写到 redo log（粉板），并更新内存，这个时候更新就算完成。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做
  - 如果某天赊账特别多，粉板写满了，又怎么办呢？ 循环双指针
    - 掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间
    - InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写
    - write pos 当前记录位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。
    - checkpoint 当前要擦除位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件
    - write pos 和 checkpoint 之间是“粉板”上空着的部分，可以用来记录新的操作
    - 如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下
  - 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe
  - innodb_flush_log_at_trx_commit 参数设置成 1 ，表示每次事务 redo log 都直接持久化到磁盘。可以保证 MySQL 异常重启之后数据不丢失

- binlog
  - statement 格式记 sql 语句
  - row 格式会记录行内容，记两条，更新前和更新后都有
    - 遇到时间，从库可能会出现不一致的情况，但是row更新前后都有，会导致日志变大
  - sync_binlog 参数设置成 1 ，表示每次事务 binlog 都持久化到磁盘。可以保证 MySQL 异常重启之后 binlog 不丢失

- `update T set c=c+1 where ID=2;` 更新内存|写入 redo log（prepare）->binlog->redo log(commit)
  - 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
  - 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据
  - 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务
  - 执行器生成这个操作的 binlog，并把 binlog 写入磁盘
  - 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成

- 两阶段提交 让两份日志之间的逻辑一致
  - 先写 redo log 再写 binlog
    - binlog 丢失 redo log 已完成记录
    - 用这个 binlog 来恢复临时库的话，缺少 redo log 丢失的执行记录，与原库值不同
  - 先写完 binlog  再写 redo log
    - 用 binlog 来恢复时候就多了事务出来
  - redo log 和 binlog 都可以用于表示事务提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致

- 可以恢复到半个月内任意一秒状态
  - 系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天|周一备
  - 一天一备跟一周一备对比
    - 最长恢复时间更短 可以恢复的最长时间
    - 一天一备模式，最坏情况下应用一天 binlog
    - 一周一备最坏情况应用一周 binlog
    - 系统对应指标 RTO 恢复目标时间
    - 更频繁全量备份需要消耗更多存储空间换来的
  - binlog 会记录所有逻辑操作，并且是采用“追加写”的形式。如果 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月所有 binlog
  - 恢复到某天中午十二点数据
    - 找到最近一次全量备份，如果运气好，可能是昨天晚上一个备份，从这个备份恢复到临时库
    - 分段回放了从备份时间点开始，将备份 binlog 依次取出来，重放到十二点之前的那个时刻
    - 把表数据从临时库取出来，按需要恢复到线上库去

- 不只是误操作后需要用这个过程来恢复数据。需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现

- 12 | 为什么我的MySQL会“抖”一下？

- 23 | MySQL是怎么保证数据不丢的？

- 24 | MySQL是怎么保证主备一致的？

- 25 | MySQL是怎么保证高可用的？

- 26 | 备库为什么会延迟好几个小时？

- 27 | 主库出问题了，从库怎么办？

- 28 | 读写分离有哪些坑？

- 29 | 如何判断一个数据库是不是出问题了？

- 31 | 误删数据后除了跑路，还能怎么办？

### 临时表

- 17 | 如何正确地显示随机消息？
- 34 | 到底可不可以使用join？
- 35 | join语句怎么优化？
- 36 | 为什么临时表可以重名？
- 37 | 什么时候会使用内部临时表？
- 43 | 要不要使用分区表？

### 实用性

- 14 | count(*)这么慢，我该怎么办？
- 32 | 为什么还有kill不掉的语句？
- 33 | 我查这么多数据，会不会把数据库内存打爆？
- 41 | 怎么最快地复制一张表？
- 44 | 答疑文章（三）：说一说这些好问题
- 45 | 自增id用完怎么办？
- 38 | 都说InnoDB好，那还要不要使用Memory引擎？
