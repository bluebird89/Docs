## [数据结构与算法之美](https://time.geekbang.org/column/intro/126) 王争

- 学习过程中碰到最大的问题，坚持不下来
  - 给自己设立一个切实可行的目标

![[../_static/ds_al.png]]

## [复杂度分析](../algorithm/algorithms.md#复杂度分析)

## 数据结构

- 线性表（Linear List）就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向
  - 数组，链表、队列、栈
- 非线性表 数据之间并不是简单的前后关系
- 特定数据结构是对特定场景的抽象

### 数组 Array

- 一种线性表数据结构。用一组连续内存空间，来存储一组具有相同类型的数据
- 实现随机访问 连续内存空间和相同类型数据
  - 数组支持随机访问，根据下标随机访问时间复杂度为 O(1)
  - 计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据
  - `a[i]_address = base_address + i * data_type_size`
  - `a[i][j] (i < m,j < n)` `address = base_address + ( i * n + j) * type_size`
  - “下标”最确切定义“偏移（offset）”
  - 从 0 开始编号实现了统一
- 连续内存空间和相同类型数据 让数组删除、插入操作变得非常低效，为保证连续性需要做大量数据搬移工作
  - JVM 标记清除垃圾回收算法 先记录下已经删除数据。每次删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，再触发执行一次真正删除操作，大大减少删除操作导致的数据搬移
- 越界问题
  - 在 C 语言中，只要不是访问受限内存，所有内存空间都是可以自由访问
    - 函数体内局部变量存在栈上，且是连续压栈。在Linux进程的内存布局中，栈区在高地址空间，从高向低增长。变量i和arr在相邻地址，且i比arr的地址大，所以arr越界正好访问到i。当然，前提是i和arr元素同类型，否则那段代码仍是未决行为
    - 栈是向下增长的，首先压栈的i，`a[2]，a[1]，a[0]`
    - 例子中死循环的问题跟编译器分配内存和字节对齐有关 数组3个元素 加上一个变量a 。4个整数刚好能满足8字节对齐 所以i的地址恰好跟着a2后面 导致死循环。。如果数组本身有4个元素 则这里不会出现死循环。。因为编译器64位操作系统下 默认会进行8字节对齐 变量i的地址就不紧跟着数组后面了
  - 数组越界在 C 语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，程序就可能不会报任何错误
- Java ArrayList
  - 优势
    - 可以将很多数组操作细节封装起来
    - 支持动态扩容
      - 数组本身在定义时需要预先指定大小，因为需要分配连续内存空间。如果申请了大小为 10 的数组，当第 11 个数据需要存储到数组中时，就需要重新分配一块更大空间，将原来数据复制过去，然后再将新数据插入
      - 使用 ArrayList，完全不需要关心底层的扩容逻辑，ArrayList 已经实现好了。每次存储空间不够的时候，都会将空间自动扩容为 1.5 倍大小
      - 扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小
  - 用数组会更合适些
    - Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组
    - 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组
    - 当要表示多维数组时，用数组往往会更加直观。比如`  Object[][] array; `用容器定义：`ArrayList<ArrayList<object> > array`

```c
// a[3] 被定位到某块不属于数组的内存地址上，而@TODO这个地址正好是存储变量 i 内存地址，那么 a[3]=0 就相当于 i=0，所以就会导致代码无限循环。
int main(int argc, char* argv[]){
    int i = 0;
    int arr[3] = {0};
    for(; i<=3; i++){
        arr[i] = 0;
        printf("hello world\n");
    }
    return 0;
}
```

### 链表 Linked list

- 通过“指针”将一组零散内存块串联起来使用
  - 内存块称为链表“结点”
  - 为将所有结点串起来，每个链表结点除了存储数据之外，还需要记录链上下一个结点地址(后继指针 next)
- 单链表
  - 头结点 第一个结点 用来记录链表基地址。有了它，可以遍历得到整条链表
  - 尾结点 最后一个结点,特殊地方：指针不是指向下一个结点，而是指向一个空地址 NULL，表示链表上最后一个结点
  - 随机访问第 k 个元素需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点 O(n)
  - 插入和删除一个数据 O(1)
- 双向链表
  - 每个结点有一个后继指针 next 指向后面结点，还有一个前驱指针 prev 指向前面结点
  - 删除值等于给定值的结点对应链表 操作总时间复杂度为 O(n)
  - 删除给定指针指向结点
    - 单链表并不支持直接获取前驱结点，为了找到前驱结点，要从头结点开始遍历链表，直到 p->next=q，说明 p 是 q 的前驱结点
    - 双向链表 O(1) 的时间复杂度
  - 在链表某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度
  - 对于一个有序链表，双向链表按值查询效率也要比单链表高一些
    - 可以记录上次查找位置 p，每次查询时，根据要查找值与 p 大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据
  - 用空间换时间 当内存空间充足时,追求代码执行速度，可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构
- 循环链表
  - 尾结点指针是指向链表的头结点
  - 优点 从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题
- 双向循环链表
- 注意
  - 理解指针或引用含义
    - “指针”还是“引用” 存储所指对象内存地址
    - 将某个变量赋值给指针，实际上将变量地址赋值给指针，或者反过来说，指针中存储变量的内存地址，指向这个变量，通过指针就能找到这个变量
  - 警惕指针丢失和内存泄漏
    - 插入结点时，一定要注意操作顺序，要先将插入结点 next 指针指向结点 b，再把结点 a 的 next 指针指向结点 x，这样才不会丢失指针，导致内存泄漏
  - 利用哨兵简化实现难度
    - 向空链表中插入第一个结点,第一个结点和其他结点插入逻辑是不一样的
    - 删除链表最后一个结点
    - head 表示头结点指针，指向链表中的第一个结点,head 指针都会一直指向哨兵结点。有哨兵结点的链表叫带头链表
    - 哨兵结点不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，可以统一为相同代码实现逻辑
  - 重点留意边界条件处理
    - 如果链表为空时，代码是否能正常工作？
    - 如果链表只包含一个结点时，代码是否能正常工作？
    - 如果链表只包含两个结点时，代码是否能正常工作？
    - 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？
  - 举例画图，辅助思考
- 链表 VS 数组
  - 数组简单易用，在实现上使用连续内存空间，可以借助 CPU 的缓存机制，预读数组中数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读
  - 数组缺点 大小固定，一经声明就要占用整块连续内存空间。如果声明数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，与数组最大区别
- 判断一个字符串是否是回文字符串
  - @TODO 快慢两个指针找到链表中点
    - 慢指针每次前进一步，快指针每次前进两步
    - 后半段链表reversed|慢指针前进过程中，同时修改其 next 指针，使得链表前半部分反序
    - 最后比较中点两侧的链表是否相等
    - 空间复杂度 O(n)
- 常见操作
  - 链表中环的检测
  - 两个有序链表合并
  - 删除链表倒数第 n 个结点
  - 求链表中间结点
  - 链表反转
- 写链表代码主要锻炼写代码能力，倒不是思考解决办法。像环检测这种解决办法想不出来
  - 实现之前的思考时间不要太长。一是先用自己能想到的暴力方法实现试试
  - 另外在一定时间内(比如半个到一个小时)实在想不到就要在网上搜搜答案
  - 有的算法，比如链表中环的检测，最优解法还是挺巧妙的，一般来说不是生想就能想到的

![[../_static/linklist_insert_delete.png]]

#### 最近最少使用策略 LRU Least Recently Used

- 操作
	- 添加
	- 删除
	- 查找
- 有序单链表 头插
  - 维护一个有序单链表，越靠近链表尾部结点是越早之前访问的
  - 访问新数据，从链表头开始顺序遍历链表
    - 已被缓存 遍历得到数据对应结点，并将其从原来位置删除，插入链表头部
    - 没有缓存
      - 缓存未满 将结点插入链表头部
      - 缓存已满 删除链表尾结点，将结点插入链表头部
- 将散列表和链表两种数据结构组合使用，时间复杂度降低到 O(1)
	- 使用双向链表存储数据，每个结点存储数据（data）、前驱指针（prev）、后继指针（next）之外，新增一个特殊字段 hnext
	- 散列表通过链表法解决散列冲突的，每个结点会在两条链中,一个是双向链表，另一个链是散列表中的拉链
	- 前驱和后继指针为了将结点串在双向链表中，hnext 指针为了将结点串在散列表的拉链中
	- 查找 
		- 散列表中查找数据的时间复杂度接近 O(1)，通过散列表可以很快地在缓存中找到一个数据
		- 找到数据之后，需要将它移动到双向链表尾部
	- 删除
		- 需要找到数据所在的结点，然后将结点删除。借助散列表可以在 O(1) 时间复杂度里找到要删除结点
		- 因为链表是双向链表，双向链表可以通过前驱指针 O(1) 时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要 O(1) 的时间复杂度
	- 添加
		- 先看这个数据是否已经在缓存中
		- 如果已经在其中，需要将其移动到双向链表尾部
		- 如果不在其中，还要看缓存有没有满
		- 如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部
		- 如果没有满，就直接将数据放到链表的尾部。
	- 涉及的查找操作通过散列表完成,其他操作比如删除头结点、链表尾部插入数据等，都可以在 O(1) 的时间复杂度内完成,时间复杂度都是 O(1)

### 跳表 Skip list

- 跳表 对链表加多级索引的结构 支持类似“二分”查找算法
- 一种各方面性能都比较优秀的动态数据结构，支持快速地插入、删除、查找操作，写起来不复杂，甚至可以替代红黑树 Red-black tree
- 对链表建立一级“索引”，每两个结点提取一个结点到上一级，抽出来那一级叫做索引或索引层
- 第一级索引结点个数大约就是 n/2，第二级索引的结点个数大约 n/4，第三级索引结点个数 n/8，依次类推，第 k 级索引结点个数是第 k-1 级索引结点个数的 1/2，第 k级索引结点个数 n/(2k)
- 查找数据 x
	- 在第 k 级索引中遍历到 y 结点后，发现 x 大于 y，小于后面结点 z
	- 通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引
	- 在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），在 K-1 级索引中最多只需要遍历 3 个结点
	- 时间复杂度 O(logn)
- 需要存储多级索引，要消耗更多的存储空
- 插入、删除操作的时间复杂度 O(logn)
	- 都需要查找到指定位置
	- 如果结点在索引中也有出现，除了要删除原始链表中结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点
- 索引动态更新
	- 如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降
	- 通过随机函数来维护“平衡性”
	- 插入数据的时候，可以选择同时将这个数据插入到部分索引层中。通过一个随机函数，来决定将这个结点插入到哪几级索引中

####  Redis 有序集合 Sorted Set

- 每个成员对象有两个属性，key（键值）和 score（分值）
- 不仅通过 score 来查找数据，还会通过 key 来查找数据
	- 与 LRU 缓存淘汰算法的解决方法类似
	- 按照键值构建一个散列表，按照 key 来删除、查找一个成员对象的时间复杂度就变成 O(1)
	- 按照分值将成员对象组成跳表结构，其他操作也非常高效
- 操作
	- 添加一个成员对象
	- 按照键值来删除一个成员对象
	- 按照键值来查找一个成员对象
	- 按照分值区间查找数据，比如查找积分在[100, 356]之间成员对象
	- 按照分值从小到大排序成员变量
- 做到 O(logn) 时间复杂度定位区间起点，然后在原始链表中顺序往后遍历就,非常高效
- 跳表更容易代码实现。虽然跳表实现也不简单，比起红黑树来说还是好懂、好写多了，而简单就意味着可读性好，不容易出错
- 跳表更加灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗
- 不能完全替代红黑树
	- 因为红黑树比跳表的出现要早一些，很多编程语言中的 Map 类型都是通过红黑树来实现的
	- 做业务开发时直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中想使用跳表，必须自己实现

### 栈 Stack

- 后进先出，先进后出
- 一种“操作受限”的线性表，只允许在一端插入和删除数据
- 当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出特性
- 操作：入栈 push()和出栈 pop()，在栈顶插入一个数据和从栈顶删除一个数据
  - 空间复杂度是 O(1)  n 个空间是必须的，只需要一两个临时变量存储空间
  - 时间复杂度都是 O(1)
- 顺序栈 用数组实现的栈
- 链式栈 用链表实现的栈
- 支持动态扩容
  - 底层依赖一个支持动态扩容的数组就可以
  - 入栈操作 当栈中有空闲空间时，时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，时间复杂度变成 O(n)，均摊时间复杂度 O(1)
- 应用
  - 函数调用栈
    - 操作系统给每个线程分配一块独立内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时临时变量
    - 每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈
    - 函数调用符合后进先出特性，用栈这种数据结构来实现是最顺理成章的选择
    - 只要能保证每进入一个新函数，都是一个新作用域就可以。实现这个，用栈就非常方便
    - 内存空间在逻辑上分为三部分
      - 代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换
      - 静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收
      - 动态数据区
        - 栈区：存储运行方法形参、局部变量、返回值。由系统自动分配和回收
        - 堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。
  - 表达式求值中 加减乘除四则运算
    - 通过两个栈实现。一个保存操作数的栈，另一个是保存运算符的栈
    - 从左向右遍历表达式，遇到数字直接压入操作数栈；遇到运算符，就与运算符栈的栈顶元素进行比较
    - 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈
    - 如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较
  - 括号匹配 给一个包含三种括号的表达式字符串，如何检查它是否合法
    - 用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，`“[”跟“]”`匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式
  - 浏览器的前进、后退功能
    - 用两个栈

```java
// 基于数组实现的顺序栈
public class ArrayStack {
  private String[] items;  // 数组
  private int count;       // 栈中元素个数
  private int n;           //栈的大小

  // 初始化数组，申请一个大小为n的数组空间
  public ArrayStack(int n) {
    this.items = new String[n];
    this.n = n;
    this.count = 0;
  }

  // 入栈操作
  public boolean push(String item) {
    // 数组空间不够了，直接返回false，入栈失败。
    if (count == n) return false;
    // 将item放到下标为count的位置，并且count加一
    items[count] = item;
    ++count;
    return true;
  }
  
  // 出栈操作
  public String pop() {
    // 栈为空，则直接返回null
    if (count == 0) return null;
    // 返回下标为count-1的数组元素，并且栈中元素个数count减一
    String tmp = items[count-1];
    --count;
    return tmp;
  }
}
```

### 队列 queue

- 一种操作受限的线性表数据结构 先进者先出
- 操作
  - 入队 enqueue()，放一个数据到队列尾部
  - 出队 dequeue()，从队列头部取一个元素
- 线程池大小设置
  - CPU 资源是有限的，任务处理速度与线程个数并不是线性正相关
  - 过多线程反而会导致 CPU 频繁切换，处理性能下降
  - 一般都是综合考虑要处理任务的特点和硬件环境，来事先设置
- 要两个指针：一个是 head 指针，指向队头；一个是 tail 指针，指向队尾
- 顺序队列 用数组实现的队列
  - 随着不停地进行入队、出队操作，head 和 tail 都会持续往后移动。当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据
  - 在入队时，集中触发一次数据搬移操作
- 链式队列 用链表实现的队列
- 循环队列
  - 避免数据搬移操作
  - 队空 head == tail
  - 队满 (tail+1)%n=head
- 阻塞队列
  - 队列为空，从队头取数据会被阻塞。没有数据可取，直到队列中有数据才能返回
  - 队列已满，插入数据操作会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回
  - 就是一个“生产者 - 消费者模型
- 并发队列
  - 线程安全的队列
  - 最简单直接实现方式直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作
  - 实际上，基于数组循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。也是循环队列比链式队列应用更加广泛原因
- 线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢
  - 非阻塞处理方式，直接拒绝任务请求
  - 阻塞处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理
    - 先进者先服务 公平地处理每个排队的请求
    - 基于链表可以实现一个支持无限排队无界队列（unbounded queue），但会导致过多请求排队等待，请求处理响应时间过长
    - 基于数组实现的有界队列（bounded queue），队列大小有限，所以线程池中排队请求超过队列大小时，接下来的请求就会被拒绝
      - 对响应时间敏感的系统来说，就相对更加合理

```java
// 用数组实现的队列
public class ArrayQueue {
  // 数组：items，数组大小：n
  private String[] items;
  private int n = 0;
  // head表示队头下标，tail表示队尾下标
  private int head = 0;
  private int tail = 0;

  // 申请一个大小为capacity的数组
  public ArrayQueue(int capacity) {
    items = new String[capacity];
    n = capacity;
  }

   // 入队操作，将item放入队尾
  public boolean enqueue(String item) {
    // tail == n表示队列末尾没有空间了
    if (tail == n) {
      // tail ==n && head==0，表示整个队列都占满了
      if (head == 0) return false;
      // 数据搬移
      for (int i = head; i < tail; ++i) {
        items[i-head] = items[i];
      }
      // 搬移完之后重新更新head和tail
      tail -= head;
      head = 0;
    }
    
    items[tail] = item;
    ++tail;
    return true;
  }

  // 出队
  public String dequeue() {
    // 如果head == tail 表示队列为空
    if (head == tail) return null;
    // 为了让其他语言的同学看的更加明确，把--操作放到单独一行来写了
    String ret = items[head];
    ++head;
    return ret;
  }
}
```

### 散列表 Hash Table

- 利用数组支持按照下标随机访问数据的特性，所以散列表是数组的一种扩展，由数组演化而来
- 通过散列函数把元素键值映射为下标，将数据存储在数组中对应下标位置
	- 键 key|关键字 标识数据
	- 散列函数|Hash 函数|哈希函数 `hash(key)` 把 key 转化为数组下标的映射方法
		- 计算得到的散列值是一个非负整数
		- 如果 key1 = key2，那 hash(key1) == hash(key2)
		- 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)
			- 几乎是不可能的 无法完全避免散列冲突，而且因为数组存储空间有限，也会加大散列冲突的概率
	- 散列值|Hash 值|哈希值 散列函数计算 key 得到散列值
- 查询 用同样散列函数将键值转化数组下标，从对应数组下标位置取数据
- 开放寻址法 open addressing
	- 出现散列冲突，重新探测一个空闲位置，将其插入
	- 线性探测 Linear Probing 
		- 插入 从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止
		- 查找 
			- 通过散列函数求出查找元素键值对应的散列值，比较数组中下标为散列值元素和要查找元素
			- 相等就是要找的元素；否则就顺序往后依次查找
			- 如果遍历到数组中空闲位置还没有找到，说明要查找元素并没有在散列表中
		- 删除
			- 将删除元素标记为 deleted。当线性探测查找遇到标记为 deleted 空间，并不是停下来而是继续往下探测
		- 问题 当散列表中插入数据越来越多时，散列冲突发生可能性就会越来越大，空闲位置会越来越少，线性探测时间就会越来越久
	- 二次探测 Quadratic probing 探测下标序列 hash(key)+0，hash(key)+1^2，hash(key)+2^2
	- 双重散列 Double hashing 不仅用一个散列函数。使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……先用第一个散列函数，如果计算得到存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲存储位置
	- 当散列表中空闲位置不多的候，散列冲突概率会大大提高。为尽可能保证散列表操作效率，尽可能保证散列表中有一定比例空闲槽位
	- 优点
		- 数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度
		- 序列化起来比较简单。链表法包含指针，序列化起来就没那么容易
	- 缺点
		- 删除数据时比较麻烦，需要特殊标记已经删除掉的数据
		- 所有数据都存储在一个数组中，比起链表法来说，冲突代价更高。装载因子上限不能太大。导致比链表法更浪费内存空间
	- 当数据量比较小、装载因子小的时候，适合采用开放寻址法。 Java 中ThreadLocalMap使用开放寻址法解决散列冲突
- 链表法 chaining
	- 每个“桶（bucket）”或者“槽（slot）”对应一条链表，所有散列值相同元素放到相同槽位对应的链表中
	- 查找、删除一个元素时，通过散列函数计算出对应的槽，遍历链表查找或者删除
	- 优点
		- 内存利用率比开放寻址法要高，链表结点可以在需要的时候再创建
		- 对大装载因子的容忍度更高，即便装载因子变成 10，也就是链表的长度变长而已
	- 缺点
		- 存储指针，对于比较小的对象的存储，比较消耗内存的，还有可能会让内存的消耗翻倍。如果存储大对象（大小远远大于一个指针的大小（4 个字节或者 8 个字节）），链表中指针内存消耗可以忽略
		- 链表中结点零散分布在内存中的，不是连续的，对 CPU 缓存是不友好的
	- 比较适合存储大对象、大数据量的散列表。比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表
- 散列表碰撞攻击 
	- 通过精心构造数据，使得所有数据经过散列函数后，都散列到同一个槽里
	- 如果用基于链表的冲突解决方法，散列表退化为链表，查询时间复杂度从 O(1) 退化为 O(n)
- 设计散列函数
	- 不能太复杂。过于复杂散列函数会消耗很多计算时间，间接地影响到散列表性能
	- 生成值要尽可能随机并且均匀分布
	- `hash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978`
- 装载因子 load factor 表示空位的多少 `散列表装载因子=填入表中元素个数/散列表长度`
- 扩容
	- 当装载因子过大时，进行动态扩容，重新申请一个更大散列表，将数据搬移到新散列表
	- 散列表扩容数据搬移操作复杂很多。因为散列表大小变了，数据存储位置也变了，需要通过散列函数重新计算每个数据的存储位置
	- 装载因子阈值设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1
	- 多步完成
		- 当装载因子触达阈值之后，只申请新空间，并不将老数据搬移到新散列表中
		- 当有新数据要插入时，将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表
		- 每次插入一个数据到散列表都重复上面的过程
		- 经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中
		- 查询 先从新散列表中查找，如果没有找到，再去老散列表中查找
- 工业级散列表
	- 要求
		- 支持快速地查询、插入、删除操作
		- 内存占用合理，不能浪费过多内存空间
		- 性能稳定，极端情况下，性能也不会退化到无法接受情况
	- 设计
		- 设计合适散列函数
		- 定义装载因子阈值，并且设计动态扩容策略
		- 选择合适散列冲突解决方法
- Java 中 HashMap
	- 默认初始大小 16，可以设置
	- 最大装载因子默认是 0.75，当 HashMap 中元素个数超过 `0.75*capacity`时启动扩容，会扩容为原来两倍大小
	- 底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能
		- 在 JDK1.8 版本中，为了对 HashMap 做进一步优化，引入红黑树
		- 当链表长度太长（默认超过 8）时，链表就转换为红黑树。利用红黑树快速增删改查的特点，提高 HashMap 的性能
		- 当红黑树结点个数少于 8 个时，将红黑树转化为链表。在数据量较小的情况下，红黑树维护平衡比起链表来，性能优势并不明显
		- 散列函数的设计并不复杂，追求的是简单高效、分布均匀 `return (h ^ (h >>> 16)) & (capicity -1);`
- 散列表和链表结合
	- 原因
		- 散列表支持非常高效数据插入、删除、查找操作，但是数据通过散列函数打乱之后无规律存储的。无法支持按照某种顺序快速地遍历数据
		- 如果希望按照顺序遍历散列表中数据，需要将散列表中数据拷贝到数组中，然后排序再遍历
		- 散列表是动态数据结构，不停地有数据插入、删除，希望按顺序遍历散列表中数据的时候，都需要先排序，那效率势必会很低
		- 为了解决这个问题， 将散列表和链表（或者跳表）结合在一起使用
	- LRU 的实现 
	- redis 的 sorted set
	- LinkedHashMap
		- Linked 指双向链表
		- 通过散列表和链表组合实现
		- 支持按照插入顺序遍历数据
		- 支持按照访问顺序遍历数据 一个支持 LRU 缓存淘汰策略的缓存系统
		- 调用 put() 函数添加数据 将数据添加到链表尾部
		- 再次将 key 为 3 数据放入，先查找键值是否已经存在，将已存在 (3,11) 删除，并且将新 (3,26) 放到链表尾部
		- 访问 key 为 5 数据，被访问数据移动到链表尾部

```java
// 10是初始大小，0.75是装载因子，true是表示按照访问时间排序
HashMap<Integer, Integer> m = new LinkedHashMap<>(10, 0.75f, true);
m.put(3, 11);
m.put(1, 12);
m.put(5, 23);
m.put(2, 22);

for (Map.Entry e : m.entrySet()) {  
	System.out.println(e.getKey());
}
// 3，1，5，2

m.put(3, 26);
m.get(5);

for (Map.Entry e : m.entrySet()) {
  System.out.println(e.getKey());
}
// 1，2，3，5
```

### 二叉树 Binary Tree

- 树 Tree
	- 父节点
	- 节点的父节点是同一个节点，之间互称为兄弟节点
	- 根节点 没有父节点的节点
	- 叶子节点|叶节点 没有子节点的节点
	- 高度（Height）节点到叶子节点最长边数，从下往上度量，从最底层开始计数，计数起点是 0
	- 深度（Depth）根节点到节点经历边数，从上往下度量，从根结点开始度量，计数起点是 0
	- 层（Level）= 深度-1 计数起点是 1，也就是说根节点位于第 1 层
	- 树高 根节点的高度
- 二叉树 每个节点最多有两个“叉”，也就是两个子节点，左子节点和右子节点
- 满二叉树 叶子节点全都在最底层，除叶子节点之外，每个节点都有左右两个子节点
- 完全二叉树 叶子节点都在最底下两层，**最后一层叶子节点都靠左排列，除最后一层，其他层的节点个数都要达到最大**
	- 求一棵包含 n 个节点的完全二叉树高度
		- `n >= 1+2+4+8+...+2^(L-2)+1`
		- `n <= 1+2+4+8+...+2^(L-2)+2^(L-1)`
		- 最大层数 L 范围 [log2(n+1), log2n +1]
- 表示|存储
	- 基于指针或者引用的二叉链式存储法
		- 每个节点有三个字段，一个存储数据，另外两个指向左右子节点的指针
		- 只要拎住根节点，通过左右子节点指针，把整棵树都串起来
	- 基于数组的顺序存储法
		- 把根节点存储在下标 i = 1 的位置，左子节点存储在下标 2 * i = 2 位置，右子节点存储在 2 * i + 1 = 3 位置
		- 下标为 i/2 位置存储它的父节点
		- 通过这种方式，只要知道根节点存储位置（一般情况下为方便计算子节点，根节点会存储在下标为 1 位置），可以通过下标计算，把整棵树都串起来
		- 非完全二叉树会浪费比较多数组存储空间
		- 完全二叉树用数组存储无疑是最节省内存的一种方式。因为数组存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针
		- 这是为什么完全二叉树会单独拎出来原因，也是为什么完全二叉树要求最后一层子节点都靠左原因

#### 遍历

- 前序遍历 对于树中任意节点，先打印这个节点，然后再打印它的左子树，最后打印它的右子树
	- `preOrder(r) = print r->preOrder(r->left)->preOrder(r->right)`
- 中序遍历 对于树中任意节点，先打印它的左子树，然后再打印它本身，最后打印它的右子树
	- `inOrder(r) = inOrder(r->left)->print r->inOrder(r->right)`
- 后序遍历 对于树中任意节点，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身
	- `postOrder(r) = postOrder(r->left)->postOrder(r->right)->print r`
- 按层遍历
	- 通过队列实现
- 遍历时间复杂度 O(n) 每个节点最多会被访问两次

![[../_static/bt_travserse.png]]

```java
void preOrder(Node* root) {
  if (root == null) return;
  print root // 此处为伪代码，表示打印root节点
  preOrder(root->left);
  preOrder(root->right);
}

void inOrder(Node* root) {
  if (root == null) return;
  inOrder(root->left);
  print root // 此处为伪代码，表示打印root节点
  inOrder(root->right);
}

void postOrder(Node* root) {
  if (root == null) return;
  postOrder(root->left);
  postOrder(root->right);
  print root // 此处为伪代码，表示打印root节点
}
```

#### 二叉查找树 Binary Search Tree

- 树中任意一个节点，左子树中每个节点值都小于这个节点值，右子树中每个节点值都大于这个节点值
- 支持快速查找一个数据、快速插入、删除一个数据
- 查找
	- 先取根节点，如果等于要查找的数据，那就返回
	- 如果要查找数据比根节点值小，在左子树中递归查找
	- 如果要查找数据比根节点值大，在右子树中递归查找
- 插入
- 删除
	- 删除节点没有子节点，直接将父节点中指向要删除节点指针置为 null
	- 删除节点只有一个子节点（只有左子节点或者右子节点），只需要更新父节点中指向要删除节点指针，让它指向要删除节点的子节点
	- 删除节点有两个子节点
		- 找到节点右子树中最小节点替换到要删除节点上
		- 删除掉最小节点，因为最小节点肯定没有左子节点
- 查找最大节点和最小节点、前驱节点和后继节点
- 中序遍历二叉查找树 可以输出有序数据序列，时间复杂度 O(n)
- 重复数据的二叉查找树
	- 在二叉查找树中存储一个多字段对象。利用对象某个字段作为键值（key）来构建二叉查找树
	- 两个对象键值相同
		- 通过链表和支持动态扩容的数组等数据结构，把值相同数据存储在同一个节点上
		- 每个节点只存储一个数据
			- 查找插入位置过程中，如果碰到一个节点值与要插入数据值相同，将要插入数据放到这个节点右子树，把新插入数据当作大于这个节点值来处理
			- 查找数据 遇到值相同节点，并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。可以把键值等于要查找值的所有节点都找出来
- 操作性能跟树高度成正比

```java
public class BinarySearchTree {
  private Node tree;

  public Node find(int data) {
    Node p = tree;
	  
    while (p != null) {
      if (data < p.data) p = p.left;
      else if (data > p.data) p = p.right;
      else return p;
    }
	  
    return null;
  }

public void insert(int data) {
  if (tree == null) {
    tree = new Node(data);
    return;
  }

  Node p = tree;
  while (p != null) {
    if (data > p.data) {
      if (p.right == null) {
        p.right = new Node(data);
        return;
      }
      p = p.right;
    } else { // data < p.data
      if (p.left == null) {
        p.left = new Node(data);
        return;
      }
      p = p.left;
    }
  }
}
  
public void delete(int data) {
  Node p = tree; // p指向要删除的节点，初始化指向根节点
  Node pp = null; // pp记录的是p的父节点
	
  while (p != null && p.data != data) {
    pp = p;
    if (data > p.data) p = p.right;
    else p = p.left;
  }
  if (p == null) return; // 没有找到

  // 要删除的节点有两个子节点
  if (p.left != null && p.right != null) { // 查找右子树中最小节点
    Node minP = p.right;
    Node minPP = p; // minPP表示minP的父节点
    while (minP.left != null) {
      minPP = minP;
      minP = minP.left;
    }
    p.data = minP.data; // 将minP的数据替换到p中
    p = minP; // 下面就变成了删除minP了
    pp = minPP;
  }

  // 删除节点是叶子节点或者仅有一个子节点
  Node child; // p的子节点
  if (p.left != null) child = p.left;
  else if (p.right != null) child = p.right;
  else child = null;

  if (pp == null) tree = child; // 删除的是根节点
  else if (pp.left == p) pp.left = child;
  else pp.right = child;
}

  public static class Node {
    private int data;
    private Node left;
    private Node right;

    public Node(int data) {
      this.data = data;
    }
  }
}
```

### [红黑树 Red-Black Tree R-B Tree](https://time.geekbang.org/column/article/68976) @todo

- 平衡二叉查找树定义 二叉树中任意一个节点左右子树的高度相差不能大于 1
- 发明初衷 解决普通二叉查找树在频繁插入、删除等动态更新情况下，出现时间复杂度退化问题
- 平衡 让整棵树左右看起来比较“对称”、“平衡”，不要出现左子树很高、右子树很矮情况。就能让整棵树高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些
- 最先被发明平衡二叉查找树 AVL 树
	- 一种高度平衡的二叉树，查找效率非常高
	- 为维持高度平衡，每次插入、删除都要做调整，就比较复杂、耗时
	- 对于频繁插入、删除操作数据集合，使用 AVL 树的代价就有点高
- 红黑树没有严格符合平衡二叉查找树定义，从根节点到各个叶子节点最长路径，有可能会比最短路径大一倍
	- 做到近似平衡，并不是严格平衡，在维护平衡成本上，比 AVL 树要低
	- 树中节点，一类被标记为黑色，一类被标记为红色
	- 根节点是黑色
	- 叶子节点是黑色空节点（NIL），不存储数据
		- 为简化红黑树代码实现而设置
	- 任何相邻节点(上下级关系)不能同时为红色，红色节点被黑色节点隔开
	- 每个节点到达其可达叶子节点的所有路径，包含相同数目黑色节点
- 近似平衡”等价为性能不会退化得太严重 要证明红黑树近似平衡 分析红黑树高度是否比较稳定趋近 log2n
	- 红色节点删除后，有些节点就没有父节点了，会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。之前的二叉树就变成四叉树
	- 单纯包含黑色节点红黑树的高度（最后一条），从四叉树中取出某些节点，放到叶节点位置，四叉树变成完全二叉树。仅包含黑色节点的四叉树高度，比包含相同节点数完全二叉树的高度 log2n还要小
	- 把红色节点加回去 红色节点不能相邻，有一个红色节点就要至少有一个黑色节点将它跟其他红色节点隔开。红黑树中包含最多黑色节点路径不会超过 log2n，所以加入红色节点后，最长路径不会超过 2log2n
- 红黑树高度比高度平衡 AVL 树的高度（log2n）大一倍，在性能上下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好
- 实现 遇到什么样节点排布，就对应怎么去调整
	- 左旋 rotate left
	- 右旋 rotate right
- 插入
	- 规定插入节点必须是红色,放在叶子节点上
	- 如果插入节点是根节点，改变它的颜色变成黑色就可以
	- 如果插入节点父节点是黑色,什么都不用做仍然满足红黑树定义
	- 除此之外，其他情况都会违背红黑树定义，需要进行调整，包含两种基础操作：左右旋转和改变颜色
	- 平衡调整是一个迭代过程
	- 正在处理节点叫关注节点,关注节点会随着不停地迭代处理而不断发生变化
		- 最开始的关注节点就是新插入节点
		- 新节点插入之后，如果红黑树平衡被打破，会有下面三种情况,只需要根据每种情况特点不停地调整，让红黑树继续符合定义，继续保持平衡,为简化描述，把父节点的兄弟节点叫做叔叔节点，父节点父节点叫做祖父节点
	- CASE 1：如果关注节点是 a，叔叔节点 d 是红色，依次执行下面操作
		- 将关注节点 a 的父节点 b、叔叔节点 d 颜色设置成黑色
		- 将关注节点 a 的祖父节点 c 颜色设置成红色
		- 关注节点变成 a 祖父节点 c
		- 跳到 CASE 2 或者 CASE 3
	- CASE 2：如果关注节点 a，叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的右子节点
		- 关注节点变成节点 a 的父节点 b
		- 围绕新关注节点 b 左旋；跳到 CASE  3
	- CASE 3：如果关注节点是 a，叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的左子节点
		- 围绕关注节点 a 的祖父节点 c 右旋
		- 将关注节点 a 的父节点 b、兄弟节点 c 颜色互换
		- 调整结束
- 删除
	- 针对删除节点初步调整
		- 初步调整只是保证整棵红黑树在一个节点删除之后，满足每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点
	- 针对关注节点进行二次调整，满足不存在相邻的两个红色节点

![[../_static/rbtree_rotate.png]]

### 堆 Heap

- 一个完全二叉树,适合用数组来存储
- 每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值
- 大顶堆
- 小顶堆
- 插入
	- 堆化 heapify 顺着节点所在路径，向上或者向下，对比交换
		- 从下往上
		- 从上往下
- 删除
	- 构造大顶堆,删除堆顶元素后，需要把第二大元素放到堆顶，出现在左右子节点中。然后再迭代地删除第二大节点，以此类推，直到叶子节点被删除.最后堆化出来的堆并不满足完全二叉树的特性(空洞)
	- 改进  把最后一个节点放到堆顶，然后利用父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止
- 插入和删除堆顶元素逻辑就是堆化，时间复杂度 O(logn)

```java
public class Heap {
  private int[] a; // 数组，从下标1开始存储数据
  private int n;  // 堆可以存储的最大数据个数
  private int count; // 堆中已经存储的数据个数

  public Heap(int capacity) {
    a = new int[capacity + 1];
    n = capacity;
    count = 0;
  }

  public void insert(int data) {
    if (count >= n) return; // 堆满了
    ++count;
    a[count] = data;
	  
    int i = count;
    while (i/2 > 0 && a[i] > a[i/2]) { // 自下往上堆化
      swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素
      i = i/2;
    }
  }
	
	public void removeMax() {
	  if (count == 0) return -1; // 堆中没有数据
	  a[1] = a[count];
	  --count;
	  heapify(a, count, 1);
	}

	private void heapify(int[] a, int n, int i) { // 自上往下堆化
	  while (true) {
		int maxPos = i;
		if (i*2 <= n && a[i] < a[i*2]) maxPos = i*2;
		if (i*2+1 <= n && a[maxPos] < a[i*2+1]) maxPos = i*2+1;
		if (maxPos == i) break;
		swap(a, i, maxPos);
		i = maxPos;
	  }
	}
 }
```

#### 优先级队列

- 数据出队顺序按照优先级来，优先级最高的最先出队
- 用堆来实现是最直接、最高效的，堆和优先级队列非常相似
	- 堆可以看作一个优先级队列，只是概念上区分而已
	- 往优先级队列中插入一个元素相当于往堆中插入一个元素
	- 从优先级队列中取出优先级最高元素相当于取出堆顶元素
- 应用
	- 多个文件中内容汇总排序
	- 高性能定时器 

#### 利用堆求 Top K

- 在一个包含 n 个数据的数组中，查找前 K 大数据
- 静态数据集合 数据集合事先确定，不会再变
	- 维护一个大小为 K 的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较
	- 如果比堆顶元素大，把堆顶元素删除，并且将这个元素插入到堆中
	- 如果比堆顶元素小，则不做处理，继续遍历数组
	- 遍历数组 O(n) 的时间复杂度，一次堆化操作需要 O(logK) 时间复杂度，最坏情况下，n 个元素都入堆一次，时间复杂度就是 O(nlogK)
- 动态数据集合 数据集合事先并不确定，有数据动态地加入到集合中
	- 一直维护一个 K 大小的小顶堆，当有数据被添加到集合中时，拿它与堆顶元素对比
	- 如果比堆顶元素大，把堆顶元素删除，并且将元素插入到堆中
	- 如果比堆顶元素小，则不做处理
	- 无论任何时候需要查询当前的前 K 大数据，都可以立刻返回给他
- 10 亿条搜索关键词日志文件，获取 Top 10 最热门搜索关键词
	- 不重复关键字有 1 亿条，如果每个搜索关键词的平均长度 50 个字节
	- 存储 1 亿个关键词起码需要 5GB 的内存空间，而散列表因为要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存空间就更多了。而机器只有 1GB 可用内存空间
	- 将 10 亿条搜索关键词先通过哈希算法分片到 10 个文件中
		- 创建 10 个空文件 00，01，02，……，09。遍历这 10 亿个关键词，并且通过某个哈希算法对其求哈希值
		- 哈希值同 10 取模，得到结果就是这个搜索关键词应该被分到的文件编号 

#### 利用堆求中位数

- 维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据
- 中位数 n 个数据
	- n 是偶数，前 n/2 数据存储在大顶堆中，后 n/2 数据存储在小顶堆中。大顶堆中堆顶元素是要找的中位数
	- n 是奇数，大顶堆就存储 n/2+1 个数据，小顶堆中就存储 n/2 个数据
- 添加数据
	- 如果新加入数据小于等于大顶堆堆顶元素，将新数据插入到大顶堆。否则，将新数据插入到小顶堆
	- 从一个堆中不停地将堆顶元素移动到另一个堆，通过调整让两个堆中数据满足数量的约定
- 类似的 求其他百分位的数据 如何快速求接口 99% 响应时间？
	- 如果有 n 个数据，从小到大排列之后，99 百分位数大约就是第 `n*99%` 个数据
	- 维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是 n，大顶堆中保存 `n*99%` 个数据，小顶堆中保存 `n*1%` 个数据。大顶堆堆顶的数据就是要找 99% 响应时间
	- 插入跟维持平衡是一样的逻辑

### 图 Graph

- 顶点 vertex 图中元素
- 边 edge 图中一个顶点可以与任意其他顶点建立连接关系
- 有向图 边有方向的图 微博
	- 入度 In-degree 有多少条边指向这个顶点
	- 出度 Out-degree 有多少条边以这个顶点为起点指向其他顶点
- 无向图 边没有方向的图 微信
	- 度 degree 跟顶点相连接的边的条数
- 带权图 weighted graph QQ 亲密度
	- 每条边有一个权重 weight
- 存储方法
	- 邻接矩阵 Adjacency Matrix
		- 底层依赖一个二维数组
		- 对于无向图，顶点 i 与顶点 j 之间有边，将 A[i][j]和 A[j][i]标记为 1
		- 对于有向图，顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，将 A[i][j]标记为 1。如果有一条箭头从顶点 j 指向顶点 i 的边，将 A[j][i]标记为 1
		- 对于带权图，数组中存储相应权重
		- 优点
			- 存储方式简单、直接，基于数组，所以在获取两个顶点的关系时，就非常高效
			- 方便计算。很多图的运算转换成矩阵之间的运算
		- 浪费存储空间
	- 稀疏图 Sparse Matrix
	- 邻接表 Adjacency List
		- 每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点
	- 微博
		- 操作
			- 判断用户 A 是否关注用户 B
			- 判断用户 A 是否是用户 B 的粉丝
			- 用户 A 关注| 取消关注用户 B
			- 根据用户名称的首字母排序，分页获取用户粉丝列表|关注列表
		- 存储
			- 邻接表中存储用户关注关系
				- 基础的邻接表不适合快速判断两个用户之间是否是关注与被关注的关系
				- 将邻接表中链表改为支持快速查找的动态数据结构,因为需要按照用户名称的首字母排序，分页来获取用户的粉丝列表或者关注列表，用跳表这种结构再合适不过
				- 跳表插入、删除、查找都非常高效，时间复杂度 O(logn)，空间复杂度上稍高，是 O(n)。最重要一点，**跳表中存储的数据本来就是有序?** ，分页获取粉丝列表或关注列表非常高效
			- 逆邻接表中存储用户被关注关系
			- 像微博那样有上亿的用户，数据规模太大无法全部存储在内存中,通过哈希算法等数据分片方式，将邻接表存储在不同的机器上.逆邻接表的处理方式也一样
			- 查询顶点与顶点关系的时候，利用同样哈希算法，先定位顶点所在机器，然后再在相应的机器上查找
		- 数据库存储
- 广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，比起其他高级的搜索算法，比如 A*、IDA* 等，要简单粗暴，没有什么优化，所以，也被叫作暴力搜索算法。仅适用于状态空间不大，图不大的搜索

```java
public class Graph { // 无向图
  private int v; // 顶点的个数
  private LinkedList<Integer> adj[]; // 邻接表

  public Graph(int v) {
    this.v = v;
    adj = new LinkedList[v];
    for (int i=0; i<v; ++i) {
      adj[i] = new LinkedList<>();
    }
  }

  public void addEdge(int s, int t) { // 无向图一条边存两次
    adj[s].add(t);
    adj[t].add(s);
  }
	
	
public void bfs(int s, int t) {
  if (s == t) return;
	
  boolean[] visited = new boolean[v];
  visited[s]=true;
  
  Queue<Integer> queue = new LinkedList<>();
  queue.add(s);
  
  int[] prev = new int[v];
  for (int i = 0; i < v; ++i) {
    prev[i] = -1;
  }
	
  while (queue.size() != 0) {
    int w = queue.poll();
   for (int i = 0; i < adj[w].size(); ++i) {
      int q = adj[w].get(i);
      if (!visited[q]) {
        prev[q] = w;
        if (q == t) {
          print(prev, s, t);
          return;
        }
        visited[q] = true;
        queue.add(q);
      }
    }
  }
}

private void print(int[] prev, int s, int t) { // 递归打印s->t的路径
  if (prev[t] != -1 && t != s) {
    print(prev, s, prev[t]);
  }
  System.out.print(t + " ");
}
	
	
boolean found = false; // 全局变量或者类成员变量

public void dfs(int s, int t) {
  found = false;
  boolean[] visited = new boolean[v];
  int[] prev = new int[v];
  for (int i = 0; i < v; ++i) {
    prev[i] = -1;
  }
  recurDfs(s, t, visited, prev);
  print(prev, s, t);
}

private void recurDfs(int w, int t, boolean[] visited, int[] prev) {
  if (found == true) return;
  visited[w] = true;
  if (w == t) {
    found = true;
    return;
  }
  for (int i = 0; i < adj[w].size(); ++i) {
    int q = adj[w].get(i);
    if (!visited[q]) {
      prev[q] = w;
      recurDfs(q, t, visited, prev);
    }
  }
}
}
```

#### 广度优先搜索 Breadth-First-Search BFS

- 先查找离起始顶点最近的，然后是次近的，依次往外搜索
- s 表示起始顶点，t 表示终止顶点。搜索一条从 s 到 t 的路径,求得的路径就是从 s 到 t 的最短路径
- visited 记录已被访问顶点，避免顶点被重复访问。如果顶点 q 被访问，相应 visited[q]被设置为 true
- queue 一个队列，用来存储已经被访问、但相连顶点还没有被访问顶点
	- 广度优先搜索是逐层访问的，只有把第 k 层顶点都访问完成之后，才能访问第 k+1 层顶点
	- 访问到第 k 层顶点时，把第 k 层顶点记录下来，稍后通过第 k 层顶点来找第 k+1 层顶点
- prev 用来记录搜索路径。从顶点 s 开始，广度优先搜索到顶点 t 后，prev 数组中存储的就是搜索的路径。路径是反向存储的。prev[w]存储顶点 w 是从哪个前驱顶点遍历过来的。比如，通过顶点 2 的邻接表访问到顶点 3，那 prev[3]就等于 2。为了正向打印出路径，需要递归地来打印  print() 函数
- 最坏情况 终止顶点 t 离起始顶点 s 很远，需要遍历完整个图才能找到
	- 每个顶点都要进出一遍队列，每个边也都会被访问一次 时间复杂度是 O(V+E) V 表示顶点个数，E 表示边个数
	- 对于一个连通图(图中所有顶点都是连通的)，E 肯定要大于等于 V-1.时间复杂度简写为 O(E)

#### 深度优先搜索 Depth-First-Search DFS

- 走迷宫 假设站在迷宫某个岔路口，然后想找到出口。随意选择一个岔路口来走，走着走着发现走不通的时候，就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口
- 用回溯思想,借助栈来实现
- 每条边最多会被访问两次，一次是遍历，一次是回退。时间复杂度 O(E)，E 表示边的个数

### Trie 树

## 思想

### 分治

- 分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了
- 分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧

### 贪心

### 回溯

### 动态规划

## 实现

### 递归 Recursion

- 看电影问你现在坐在第几排
	- 问前面一排的人他是第几排，只要在他的数字上加一，就知道自己在哪一排了
	- 前面的人也看不清啊，所以他也问他前面的人
	- 就这样一排一排往前问，直到问到第一排的人，说我在第一排，然后再这样一排一排再把数字传回来
	- 直到你前面的人告诉你他在哪一排
	- 是一个非常标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”
- 所有递归问题可以用递推公式来表示，满足条件
  - 一个问题的解可以分解为子问题的解
  - 这个问题与分解之后的子问题，除数据规模不同，求解思路完全一样
  - 存在递归终止条件
- 写递归代码
  - **如何将大问题分解为小问题的规律**，并基于此写出递推公式
  - 推敲终止条件
  - 将递推公式和终止条件翻译成代码
  - 编写递归代码关键 只要遇到递归，就抽象成一个递推公式，不用想一层层调用关系，不要试图用人脑去分解递归每个步骤
- 归纳法
  - 思维误区
    - 人脑几乎没办法把整个“递”和“归”过程一步一步都想清楚
    - 把递归平铺展开，脑子里会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，很容易被绕进去
  - 正确思维方式
    - 如果一个问题 A 可以分解为若干子问题 B、C、D，可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A
    - 只需要思考问题 A 与子问题 B、C、D 两层之间关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间关系。屏蔽掉递归细节
- 利 表达力很强，写起来简洁
- 弊 空间复杂度高 过多函数调用会耗时较多
  - 堆栈溢出
	  - 在代码中限制递归调用最大深度，超过一定深度（比如 1000）之后就不继续往下再递归了，直接返回报错
  - 重复计算 通过数据结构（比如散列表）保存已经求解过 f(k)
- 时间复杂度
	- 递推公式 `T(n)=2T(2n​)+n`
	- 递归树
		- 快速排序每次分区大小比例为 1:9
			- 递归树并不是满二叉树
			- 快速排序结束条件是待排序的小区间为 1，也就是叶子节点里数据规模是 1
			- 从根节点 n 到叶子节点 1，最短路径每次都乘以 1/10，最长路径每次都乘以 9/10
			- 通过计算，从根节点到叶子节点最短路径 log10n，最长路径 log9/10n
			- 对数复杂度底数不管是多少，统一写成 logn
	- 斐波那契数列
		- 每次数据规模是 −1 或者 −2，叶子节点数据是 1 或者 2
		- 从根节点走到叶子节点，最长路径 n,最短路径大约 n/2
		- 每次分解之后合并操作需要一次加法运算，时间消耗记作 1,第 k 层时间消耗 2^(k−1),做求和
		- 时间复杂度介于 O(2^n) 和 O(2^(n/2) 之间
	- 全排列 数组中存储1，2， 3...n。
        - `f(1,2,...n) = {最后一位是1, f(n-1)} + {最后一位是2, f(n-1)} +...+{最后一位是n, f(n-1)}`
        - 第 k 层交换次数 `n∗(n−1)∗(n−2)∗...∗(n−k+1)`
- 递归代码=>非递归代码
  - 根据实际情况选择是否需要用递归方式来实现
  - 递归本身借助系统或者虚拟机本身提供的栈来实现。如果自己在内存堆上实现栈，手动模拟入栈、出栈过程，任何递归代码都可以改写成看上去不是递归代码
  - 实际将递归改为“手动”递归，本质并没有变，而且也并没有解决前面讲到某些问题，徒增实现复杂度
- 调试
  - 打印日志发现，递归值
  - 结合条件断点进行调试
- 1 个细胞的生命周期是 3 小时，1 小时分裂一次。求 n 小时后，容器内有多少细胞？

```java
public int f(int n) {
  if (n == 1) return 1;
  if (n == 2) return 2;
  
  // hasSolvedList可以理解成一个Map，key是n，value是f(n)
  if (hasSolvedList.containsKey(n)) {
    return hasSolvedList.get(n);
  }
  
  int ret = f(n-1) + f(n-2);
  hasSolvedList.put(n, ret);
  return ret;
}


int f(int n) {
  if (n == 1) return 1;
  if (n == 2) return 2;
  
  int ret = 0;
  int pre = 2;
  int prepre = 1;
  for (int i = 3; i <= n; ++i) {
    ret = pre + prepre;
    prepre = pre;
    pre = ret;
  }
  return ret;
}


// 调用方式：int[]a = a={1, 2, 3, 4}; printPermutations(a, 4, 4);
// k表示要处理的子数组的数据个数
public void printPermutations(int[] data, int n, int k) {
  if (k == 1) {
    for (int i = 0; i < n; ++i) {
      System.out.print(data[i] + " ");
    }
    System.out.println();
  }

  for (int i = 0; i < k; ++i) {
    int tmp = data[i];
    data[i] = data[k-1];
    data[k-1] = tmp;

    printPermutations(data, n, k - 1);

    tmp = data[i];
    data[i] = data[k-1];
    data[k-1] = tmp;
  }
}
```

### 排序

- 线性排序（Linear sort）时间复杂度是 O(n)，是线性的，桶排序、计数排序、基数排序
- 分析
  - 执行效率
    - 最好情况、最坏情况、平均情况时间复杂度,最好、最坏时间复杂度对应的要排序的原始数据是什么样
    - 时间复杂度的系数、常数 、低阶 在对同一阶时间复杂度的排序算法性能对比时要把系数、常数、低阶也考虑进来
  - 内存消耗
    - 原地排序（Sorted in place）特指空间复杂度是 O(1) 排序算法
  - 稳定性 如果待排序序列中存在值相等元素，经排序后相等元素之间原有顺序不变
    - 排序的是一组对象，按照对象某个 key 来排序
    - 给电商交易系统中10 万条订单数据排序。订单有两个属性，一个是下单时间，另一个是订单金额。按照金额从小到大对订单数据排序。对于金额相同订单按照下单时间从早到晚有序
      - 先按照金额对订单数据进行排序，然后再遍历排序后的订单数据，对于每个金额相同小区间再按照下单时间排序
      - 先按照下单时间给订单排序，排序完成之后，用稳定排序算法，按照订单金额重新排序
        - 稳定排序算法可以保持金额相同对象在排序后的前后顺序不变
- 有序度 数组中具有有序关系的元素对个数
  - 有序元素对 `a[i] <= a[j]` 如果i < j
  - 满有序度 完全有序数组 有序度 `n*(n-1)/2`
- 逆序度定义 跟有序度相反（默认从小到大为有序）
  - 逆序元素对 `a[i] > a[j]`, 如果i < j
  - 逆序度 = 满有序度 - 有序度
- 排序过程 一种增加有序度，减少逆序度过程，最后达到满有序度
- 外部排序 数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中
- 对小规模数据选择时间复杂度 O(n2) 算法；对大规模数据，时间复杂度 O(nlogn) 的算法更加高效。为兼顾任意规模数据排序，一般首选时间复杂度 O(nlogn) 来实现
  - 堆排序和快速排序都有比较多的应用，比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数
- 归并排序 不是原地排序算法，空间复杂度 O(n)
- qsort()
  - 优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是 O(n)，所以对于小数据量的排序，比如 1KB、2KB 等，归并排序额外需要 1KB、2KB 的内存空间问题不大
  - 要排序数据量比较大的时候，改为用快速排序
  - 在快速排序过程中，当要排序区间中元素个数小于等于 4 时，退化为插入排序，不再继续用递归来做快速排序

| 方法 | 是否原地 | 是否稳定 | 最好       | 最坏       | 平均       |
| -- | ---- | ---- | -------- | -------- | -------- |
| 冒泡 | yes  | yes  | O(n)     | O(n^2)   | O(n^2)   |
| 插入 | yes  | yes  | O(n)     | O(n^2)   | O(n^2)   |
| 选择 | yes  | no   | O(n^2)   | O(n^2)   | O(n^2)   |
| 归并 | no   | yes  | O(nlogn) | O(nlogn) | O(nlogn) |
| 快速 | yes  | no   |          | O(n^2)   | O(nlogn) |
| 计数 | no   | yes  |          |          | O(n+k)   |
| 基数 | no   | yes  |          |          | O(n)     |
| 桶  | no   | yes  |          |          | O(dn)    |

#### 冒泡排序 Bubble

- 未排序区最大值**冒出来**放到排序区
  - 冒泡 只会操作相邻两个数据。每次冒泡操作都会对相邻两个元素进行比较，不满足就互换
  - 一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，完成 n 个数据的排序
- 优化 当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操
- 原地排序算法 空间复杂度 O(1)
- 稳定排序算法 相邻元素大小相等时不做交换 相同大小数据在排序前后不会改变顺序
- 最好情况时间复杂度 O(n) 要排序的数据已经有序，只需要进行一次冒泡操作，就可以结束
- 最坏情况时间复杂度 O(n^2) 要排序数据刚好是倒序排列的，需要进行 n 次冒泡操作
- 平均时间复杂度  O(n2)
  - 冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即逆序度 = `n*(n-1)/2`–初始有序度
  - 平均情况下，需要 `n*(n-1)/4` 次交换操作

```java
// 冒泡排序，a表示数组，n表示数组大小
public void bubbleSort(int[] a, int n) {
  if (n <= 1) return;
 
 for (int i = 0; i < n; ++i) {
    // 提前退出冒泡循环的标志位
    boolean flag = false;
    for (int j = 0; j < n - i - 1; ++j) {
      if (a[j] > a[j+1]) { // 交换
        int tmp = a[j];
        a[j] = a[j+1];
        a[j+1] = tmp;
        flag = true;  // 表示有数据交换      
      }
    }
	 
    if (!flag) break;  // 没有数据交换，提前退出
  }
}
```

#### 插入排序 Insertion

- 将未排序区数据**插入到**已排序区
  - 将数组中数据分为两个区间，已排序区间和未排序区间
  - 初始已排序区间只有一个元素，数组第一个元素
  - 取未排序区间中元素，在已排序区间中找到合适插入位置将其插入，保证已排序区间数据一直有序
  - 重复这个过程，直到未排序区间中元素为空，算法结束
- 将数据 a 插入到已排序区间时，需要拿 a 与已排序区间元素依次比较大小，找到合适插入位置。找到插入点后，需要将插入点之后元素顺序往后移动一位，腾出位置给元素 a 插入
  - 不同的查找插入插入点方法（从头到尾、从尾到头),移动次数总是固定的，等于逆序度
- 原地排序算法 空间复杂度是 O(1)
- 稳定排序算法
- 最好时间复杂度为 O(n) 要排序数据已经有序,每次只需要比较一个数据就能确定插入的位置.从尾到头遍历已经有序数据
- 最坏情况时间复杂度 O(n2) 数组是倒序的，每次插入都相当于在数组第一个位置插入新的数据
- 为什么插入排序要比冒泡排序更受欢迎
  - 冒泡排序的数据交换比插入排序的数据移动复杂，冒泡排序需要 3 个赋值操作，而插入排序需要 1 个

```java
// 插入排序，a表示数组，n表示数组大小
public void insertionSort(int[] a, int n) {
  if (n <= 1) return;

  for (int i = 1; i < n; ++i) {
    int value = a[i];
    int j = i - 1;
    // 查找插入的位置
    for (; j >= 0; --j) {
      if (a[j] > value) {
        a[j+1] = a[j];  // 数据移动
      } else {
        break;
      }
    }
    a[j+1] = value; // 插入数据
  }
}
```

#### 选择排序 Selection

- 每次从未排序区间中选择最小元素(选择)放到已排序区间末尾
- 空间复杂度为 O(1) 一种原地排序算法
- 最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n2)
- 不稳定的排序算法

#### 归并排序 Merge

- 把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序两部分合并在一起，整个数组就都有序
  - 拆分到单个元素，merge 逻辑是通用的
- 递推公式 `merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))`
- 终止条件 `p >= r` 不用再继续分解
- 借助哨兵
- 稳定排序算法
- 时间复杂度
  - 对 n 个元素进行归并排序需要时间 T(n)，分解成两个子数组排序时间都是 T(n/2)
  - merge() 函数合并两个有序子数组的时间复杂度是 O(n)
  - `T(n) = 2*T(n/2) + n=2^k * T(n/2^k) + k * n`
  - 当 T(n/2^k)=T(1) 时，也就是 n/2^k=1，得到 k=log2n
  - T(n) = O(nlogn)
  - 与要排序原始数组的有序程度无关，时间复杂度非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)
- 不是原地排序
  - 在合并两个有序数组为一个有序数组时，需要借助额外存储空间
  - 空间复杂度 O(n) 每次合并操作都需要申请额外内存空间，合并完成之后，临时开辟内存空间就被释放掉。在任意时刻，CPU 只会有一个函数在执行，只会有一个临时内存空间在使用。临时内存空间最大也不会超过 n 个数据大小
- 类似后续遍历

![[../_static/merge_sort.png]]

```java
// 归并排序算法, A是数组，n表示数组大小
merge_sort(A, n) {
  merge_sort_c(A, 0, n-1)
}

// 递归调用函数
merge_sort_c(A, p, r) {
  // 递归终止条件
  if p >= r  then return

  // 取p到r之间的中间位置q
  q = (p+r) / 2
  // 分治递归
  merge_sort_c(A, p, q)
  merge_sort_c(A, q+1, r)
  // 将A[p...q]和A[q+1...r]合并为A[p...r]
  merge(A[p...r], A[p...q], A[q+1...r])
}

merge(A[p...r], A[p...q], A[q+1...r]) {
  var i := p，j := q+1，k := 0 // 初始化变量i, j, k
  var tmp := new array[0...r-p] // 申请一个大小跟A[p...r]一样的临时数组
  while i<=q AND j<=r do {
    if A[i] <= A[j] {
      tmp[k++] = A[i++] // i++等于i:=i+1
    } else {
      tmp[k++] = A[j++]
    }
  }
  
  // 判断哪个子数组中有剩余的数据
  var start := i，end := q
  if j<=r then start := j, end:=r
  
  // 将剩余的数据拷贝到临时数组tmp
  while start <= end do {
    tmp[k++] = A[start++]
  }
  
  // 将tmp中的数组拷贝回A[p...r]
  for i:=0 to r-p do {
    A[p+i] = tmp[i]
  }
}

T(n) = 2*T(n/2) + n
     = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n
     = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n
     = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n
     ......
     = 2^k * T(n/2^k) + k * n
     ......
```

#### 快速排序 Quick

- 实现
  - 排序下标从 p 到 r 之间一组数据，选择 p 到 r 之间任意一个数据作为 pivot（分区点）
  - 遍历 p 到 r 之间数据，小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间
  - 数组 p 到 r 之间数据被分成三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间 pivot，后面的 q+1 到 r 之间是大于 pivot 的
  - 可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1
  - 递推公式 `quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)`
  - 终止条件 `p >= r`
- partition() 分区函数 选择一个元素作为 pivot（一般情况下选择 p 到 r 区间最后一个元素），然后对 `A[p...r]`分区，函数返回 pivot 下标
- 不稳定排序算法 分区过程涉及交换操作，如果数组中有两个相同的元素，相对先后顺序就会改变
- 快排和归并区别
  - 归并处理过程是由下到上的，先处理子问题，然后再合并
  - 快排处理过程是由上到下的，先分区，然后再处理子问题
  - 归并排序是稳定的、时间复杂度为 O(nlogn) 排序算法，是非原地排序算法。之所以是非原地排序算法，主要原因是合并函数无法在原地执行
  - 快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题
- 最好情况时间复杂度 O(nlogn) 分区选择 pivot 很合适，正好能将大区间对等地一分为二
- 最坏情况时间复杂度  O(n2) 分区极其不均衡
- 平均情况时间复杂度  O(nlogn)
- 有 10 个接口访问日志文件，每个日志文件大小约 300MB，每个文件里的日志都是按照时间戳从小到大排序的。你希望将这 10 个较小的日志文件，合并为 1 个日志文件，合并之后的日志仍然按照时间戳从小到大排列。如果处理上述排序任务的机器内存只有 1GB
  - 每次从各个文件中取一条数据，在内存中根据数据时间戳构建一个最小堆，然后每次把最小值给写入新文件，同时将最小值来自的那个文件再出来一个数据，加入到最小堆中。这个空间复杂度为常数，但没能很好利用1g内存，而且磁盘单个读取比较慢，所以考虑每次读取一批数据，没了再从磁盘中取，时间复杂度还是一样O(n)
- 优化
  - O(n2) 时间复杂度出现原因 分区点选得不够合理
    - 数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据
    - 粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，肯定会出现
  - 最理想分区点 被分区点分开的两个分区中，数据数量差不多
    - 取出区间首、尾、中间，取中间值作为分区点
    - 每次从要排序的区间中，随机选择一个元素作为分区点

![[../_static/quicksort.jpg]]
![[../_static/mergevsquick.jpg]]

```java
// 快速排序，A是数组，n表示数组的大小
quick_sort(A, n) {
  quick_sort_c(A, 0, n-1)
}

// 快速排序递归函数，p,r为下标
quick_sort_c(A, p, r) {
  if p >= r then return
  
  q = partition(A, p, r) // 获取分区点
  quick_sort_c(A, p, q-1)
  quick_sort_c(A, q+1, r)
}

partition(A, p, r) {
  pivot := A[r]
  i := p
  for j := p to r-1 do {
    if A[j] < pivot {
      swap A[i] with A[j]
      i := i+1
    }
  }
  swap A[i] with A[r]
  return i
```

#### 桶排序 Bucket

- 核心思想
  - 将排序数据分到几个有序桶里，每个桶里数据再单独进行排序
  - 桶内排完序后，再把每个桶里数据按照顺序依次取出，组成的序列就是有序的了
- 时间复杂度 O(n)
  - 数据 n 个，均匀地划分到 m 个桶内，桶内使用快速排序 `O(m * k * logk)=O(n*log(n/m)) k=n/m`
- 要排序数据要求
  - 排序数据需要很容易就能划分成 m 个桶，并且桶与桶之间有着天然大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序
  - 数据在各个桶之间分布比较均匀
- 适合 外部排序 有 10GB 订单数据，按订单金额（假设金额都是正整数）进行排序，内存只有几百 MB
  - 先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描后得到，订单金额最小是 1 元，最大是 10 万元
  - 将所有订单根据金额划分到 100 个桶里，第一个桶存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推
  - 每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02...99）
  - 理想情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，就可以将这 100 个小文件依次放到内存中，用快排来排序
  - 等所有文件都排好序之后，按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。
  - 订单按照金额在 1 元到 10 万元之间并不一定是均匀分布的 ，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存
    - 针对这些划分之后还是比较大的文件，可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元....901 元到 1000 元。如果划分之后，101 元到 200 元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止

#### 计数排序 Counting

- 桶排序的一种特殊情况。要排序 n 个数据，所处范围并不大的时候，比如最大值是 k可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉桶内排序时间
- 假设 8 个考生，分数在 0 到 5 分之间。成绩放数组 A[8]= [2，5，3，0，2，3，0，3]
- 使用大小为 6 数组 C[6]表示桶，下标对应分数，存储小于等于分数 k 的考生个数 C[6] = [2,2,4,7,7,8]
- 从后到前扫描数组 A。当扫描到 3 时，从数组 C 中取出下标为 3 值 7，到目前为止，包括自己在内，分数小于等于 3 的考生有 7 个，也就是说 3 是数组 R 中的第 7 个元素（也就是数组 R 中下标为 6 的位置）。当 3 放入到数组 R 中后，小于等于 3 元素剩下 6 个，相应 C[3]要减 1，变成 6,得到 R[8] = [0,0,2,2,3,3,3,5]
- 只能用在数据范围不大场景中，如果数据范围 k 比要排序的数据 n 大很多不适合用计数排序
- 只能给非负整数排序，如果要排序数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数

#### 基数排序 Radix

- 有 10 万个手机号码，将手机号码从小到大排序
  - 要比较两个手机号码 a，b 大小，如果在前面几位中，a 手机号码已经比 b 手机号码大了，那后面的几位就不用看了
  - 先按照最后一位来排序手机号码，然后再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序后，手机号码就都有序了
  - 按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的
  - 时间复杂度 O(n)
- 要排序的数据并不都是等长的
  - 把所有的单词补齐到相同长度，位数不够的可以在后面补“0”
  - 所有字母都大于“0”，所以补“0”不会影响到原有的大小顺序
- 对排序数据要求：可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了
- 每一位数据范围不能太大，要可以用线性排序算法来排序，否则，时间复杂度无法做到 O(n) 

#### 堆排序

- 时间复杂度 O(nlogn)
- 原地排序算法
- 不是稳定的排序算法 排序过程自己中存在将堆最后一个节点跟堆顶节点互换操作，就有可能改变值相同数据的原始相对顺序
- 建堆 将数组原地建成一个堆
	- 堆中插入元素思路 从前往后处理数组数据，插入时是从下往上堆化
		- 假设起初堆中只包含一个数据下标为 1 数据。调用插入操作，将下标从 2 到 n 数据依次插入到堆中
	- 从后往前处理数组，每个数据都是从上往下堆化
		- 直接从最后一个非叶子节点开始（下标从 n/2 开始到 1 数据），依次堆化
	- 时间复杂度 O(n)
- 排序
	- 类似“删除堆顶元素”操作 从后往前，依次取堆顶元素，剩下堆化
	- 堆顶跟最后一个元素(下标 n 位置)交换,然后再通过堆化的方法，将剩下 n−1 个元素重新构建成堆
	- 堆化完成后，再取堆顶元素，放到下标是 n−1 位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素
	- 时间复杂度 O(nlogn)
- 堆中数据从数组下标 0 开始存储，实际上处理思路是没有任何变化的，唯一变化计算子节点和父节点的下标的公式改变了。如果节点下标 i，左子节点下标 2∗i+1，右子节点下标 2∗i+2，父节点下标 （i−1）/2
- 快速排序比堆排序性能好
	- 堆排序数据访问方式没有快速排序友好，堆排序的堆化，数据是跳着访问的，不像快速排序那样，局部顺序访问，这样对 CPU 缓存不友好的
	- 对于同样数据，堆排序数据交换次数多于快速排序

![[../_static/heapsort_t.png]]

```java
private static void buildHeap(int[] a, int n) {
  for (int i = n/2; i >= 1; --i) {
    heapify(a, n, i);
  }
}

private static void heapify(int[] a, int n, int i) {
  while (true) {
    int maxPos = i;
    if (i*2 <= n && a[i] < a[i*2]) maxPos = i*2;
    if (i*2+1 <= n && a[maxPos] < a[i*2+1]) maxPos = i*2+1;
    if (maxPos == i) break;
    swap(a, i, maxPos);
    i = maxPos;
  }
}

// n 表示数据的个数，数组a中的数据从下标1到n的位置。
public static void sort(int[] a, int n) {
  buildHeap(a, n);
  int k = n;
  while (k > 1) {
    swap(a, 1, k);
    --k;
    heapify(a, k, 1);
  }
}
```

### 二分查找 Binary Search

- 针对一个有序数据集合,每次通过跟区间中间元素对比，将待查找区间缩小为之前一半，直到找到要查找元素，或者区间被缩小为 0
- 时间复杂度  O(logn)
- 容易出错地方
  - 循环退出条件 low<=high，不是 `low<high`
  - mid 取值 `low+((high-low)>>1`
  - low 和 high 更新 `low=mid+1，high=mid-1`
  - 返回值选择
- 局限
  - 依赖顺序表结构（数组），要求内存空间连续，对内存的要求比较苛刻
  - 针对有序数据，只能用在插入、删除操作不频繁，一次排序多次查找的场景中
  - 数据量太小不适合二分查找
    - 如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找
- 凡是用二分查找能解决的，绝大部分更倾向于用散列表或者二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多
- 变体一：查找存在重复数据的数据集合中第一个值等于给定值的元素
  - 当 a[mid] 等于要查找的值时，就需要确认一 a[mid]是不是第一个值等于给定值的元素
  - 如果 mid 等于 0，那这个元素已经是数组第一个元素，肯定是要找的
  - 如果 mid 不等于 0，但 a[mid]前一个元素 a[mid-1]不等于 value，也说明 a[mid]就是要找的第一个值等于给定值的元素
  - 如果检查后发现 a[mid]前面一个元素 a[mid-1]也等于 value，说明此时 a[mid]肯定不是要查找的第一个值等于给定值的元素。更新 high=mid-1，因为要找的元素肯定出现在[low, mid-1]之间
- 变体二：查找存在重复数据的数据集合中最后一个值等于给定值的元素
- 变体三：查找第一个大于等于给定值的元素
- 变体四：查找最后一个小于等于给定值的元素

### 哈希 Hash

- 将任意长度二进制值串映射为固定长度二进制值串，映射规则就是哈希算法
- 哈希值 通过原始数据映射之后得到的二进制值串
- 要求
	- 单向 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）
	- 对输入数据非常敏感，哪怕原始数据只修改一个 Bit，最后得到哈希值也大不相同
	- 散列冲突概率要很小，对于不同原始数据，哈希值相同概率非常小
		- 基于组合数学中一个非常基础理论鸽巢原理|抽屉原理 如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个，肯定有 2 只鸽子在 1 个鸽巢内
		- 存在散列冲突情况，因为哈希值范围很大，冲突的概率极低，所以相对来说还是很难破解的。像 MD5，有 2^128 个不同的哈希值，散列冲突概率要小于 1/2^12
	- 执行效率要尽量高效，针对较长文本，也能快速地计算出哈希值
- 安全加密
	- 最常用加密哈希算法
		- MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法）
			- 哈希值是 128 位 Bit 长度
		- SHA（Secure Hash Algorithm，安全散列算法）
		- DES（Data Encryption Standard，数据加密标准）
		- AES（Advanced Encryption Standard，高级加密标准）
	- 越复杂、越难破解的加密算法，需要计算时间也越长。比如 SHA-256 比 SHA-1 要更复杂、更安全，相应计算时间就会比较长
	- 针对字典攻击
		- 维护一个常用密码字典表，字典中每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对。如果相同，基本上可以认为，加密之后密码对应的明文就是字典中的这个密码
		- 引入一个盐（salt），跟用户密码组合在一起，增加密码复杂度。拿组合之后的字符串来做哈希算法加密，存储到数据库中，进一步增加破解的难度
- 唯一标识
	- 任何文件在计算中都可以表示成二进制码串
	- 给每一个图片取一个唯一标识，或者说信息摘要
	- 继续提高效率，把每个图片的唯一标识，和相应图片文件在图库中路径信息，存储在散列表中
- 数据校验
	- BT 下载的文件块可能不是完整的。如果没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒
	- 通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。只要文件块内容有一丁点儿改变，最后计算出的哈希值就会完全不同
	- 当文件块下载完成之后，可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对
	- 如果不同，说明文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块
- 散列函数
	- 散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，都可以通过开放寻址法或者链表法解决
	- 散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布
	- 散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率
- 负载均衡
	- 替代映射表
	- 实现一个会话粘滞（session sticky）的负载均衡算法
	- 对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号
- 数据分片
	- 有 1T 日志文件，记录用户的搜索关键词，快速统计出每个关键词被搜索次数
		- 先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度
		- 为提高处理速度，用 n 台机器并行处理
		- 从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值就是应该被分配到的机器编号
		- 哈希值相同的搜索关键词被分配到同一个机器。同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现次数，最后合并起来就是最终结果
	- 判断图片是否在1 亿张图片图库中
		- 准备 n 台机器，让每台机器只维护某一部分图片对应的散列表
		- 每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表
		- 要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找
		- 散列表中每个数据单元包含哈希值和图片文件的路径。假设通过 MD5 来计算哈希值，那长度就是 128 比特，也就是 16 字节。文件路径长度上限 256 字节，可以假设平均长度 128 字节。如果用链表法来解决冲突，还需要存储指针，指针只占用 8 字节。所以，散列表中每个数据单元就占用 152 字节
		- 假设一台机器的内存大小为 2GB，散列表装载因子为 0.75，那一台机器可以给大约 1000 万`（2GB*0.75/152）`张图片构建散列表
		- 如果要对 1 亿张图片构建索引，需要大约十几台机器。在工程中，这种估算还是很重要的，能让事先对需要投入的资源、资金有个大概的了解，能更好地评估解决方案的可行性
- 分布式存储
	- 分布式缓存 通过哈希算法对数据取哈希值，然后对机器个数取模，最终值就是应该存储的缓存机器编号
	- 扩容
		- 所有数据都要重新计算哈希值，然后重新搬移到正确的机器上。缓存中数据一下子就都失效。所有数据请求都会穿透缓存，直接去请求数据库。可能发生雪崩效应，压垮数据库
	- 一致性哈希算法
		- 有 k 个机器，数据哈希值范围[0, MAX]。将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间
		- 当有新机器加入，将某几个小区间数据，从原来机器中搬移到新机器中
		- 既不用全部重新哈希、搬移数据，也保持各个机器上数据数量的均衡
		- 借助一个虚拟的环和虚拟结点，更加优美地实现出来

```sh
# 交互式输入
openssl 
OpenSSL> md5
123456(stdin)= e10adc3949ba59abbe56e057f20f883e  #输入完后按Crtl+D三次

echo 123456 |openssl md5 # MD5计算时包含了回车符
(stdin)= f447b20a7fcbf53a5d5be013ea0b15af
echo -n 123456 |openssl md5  #-n 不带回车符才是正确的
(stdin)= e10adc3949ba59abbe56e057f20f883e

openssl md5 install.log
md5sum install.log

echo abc |openssl base64  #加密
YWJjCg==

echo YWJjCg== |openssl base64 -d  # 解密
abc

openssl base64 -in test.log #加密
MTIzNDU2Cg==
openssl base64 -in test.log  >>test.log2
openssl base64 -d -in test.log2  #解密
123456

# md5/sha1摘要
echo 'abc' |openssl md5
(stdin)= 0bee89b07a248e27c83fc3d5951213c1
echo 'abc' |openssl sha1
(stdin)= 03cfd743661f07975fa2f1220c5194cbaff48451

openssl sha1 test.log
SHA1(test.log)= c4f9375f9834b4e7f0a528cc65c055702bf5f24a
openssl md5 test.log
MD5(test.log)= f447b20a7fcbf53a5d5be013ea0b15af
cat test.log
123456

# AES/DES3加密解密 对字符串‘abc’进行aes加密，使用密钥123，输出结果以base64编码格式给出：
echo 'abc' |openssl aes-128-cbc -k 123 -base64
U2FsdGVkX1/hVjxSVVQYBKYJ3VYttcoKT6dwX7z35Xk=

echo 'U2FsdGVkX1/hVjxSVVQYBKYJ3VYttcoKT6dwX7z35Xk=' |openssl aes-128-cbc -d -k 123 -base64
abc
```

### 字符串匹配

- 主串和模式串 在字符串 A 中查找字符串 B，字符串 A 是主串，字符串 B 是模式串。主串长度记作 n，模式串长度记作 m。在主串中查找模式串，所以 n>m
- Brute Force BF 暴力匹配算法|朴素匹配算法
	- 在主串中检查起始位置分别是 0、1、2....n-m 且长度为 m 的 n-m+1 个子串，有没有跟模式串匹配
	- 最坏时间复杂度 O(n*m)
- Rabin-Karp RK  由两位发明者 Rabin 和 Karp 名字来命名
	- 通过哈希算法对主串中 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小
	- 如果某个子串哈希值与模式串相等，说明对应子串和模式串匹配（先不考虑哈希冲突问题，）
	- 因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了
	- 哈希算法设计 假设要匹配字符串的字符集中包含 K 个字符，可以用一个 K 进制数来表示一个子串，这个 K 进制数转化成十进制数，作为子串哈希值
		- 有一个特点，在主串中，相邻两个子串的哈希值的计算公式有一定关系.相邻子串 s[i-1]和 s[i]（i 表示子串在主串中的起始位置），对应哈希值计算公式有交集，可以使用 s[i-1]的哈希值很快的计算出 s[i]的哈希值
	- 时间复杂度  O(n)
	- 哈希值就可能很大，超过了计算机中整型数据可以表示范围
		- 为了能将哈希值落在整型数据范围内，可以允许哈希冲突 
- Boyer-Moore BM 
	- 当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后多滑动几位
	- 坏字符规则 bad character rule
		- 按照模式串下标从大到小顺序倒着匹配
		- 当发现某个字符没法匹配的时候，把这个没有匹配的字符叫作坏字符（主串中的字符）
		- 坏字符在模式串中是否存在
	- 好后缀规则 good suffix shift
- KMP